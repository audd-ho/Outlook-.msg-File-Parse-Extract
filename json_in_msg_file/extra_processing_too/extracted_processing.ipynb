{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-cursor Codes, no zero classifier yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall Useful Functions\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "def get_length(embedding_1d):\n",
    "    sum = 0\n",
    "    for i in embedding_1d:\n",
    "        sum+=(i**2)\n",
    "    return math.sqrt(sum)\n",
    "def normalise_embedding(embedding_1d):\n",
    "    length = get_length(embedding_1d)\n",
    "    for i in range(len(embedding_1d)):\n",
    "        embedding_1d[i] /= length\n",
    "def get_normalise_embedding(embedding_1d):\n",
    "    if type(embedding_1d) is torch.Tensor:\n",
    "        temp_embedding_1d = (embedding_1d.detach().numpy()).copy()\n",
    "    else:\n",
    "        temp_embedding_1d = embedding_1d.copy()\n",
    "    length = get_length(temp_embedding_1d)\n",
    "    for i in range(len(temp_embedding_1d)):\n",
    "        temp_embedding_1d[i] /= length\n",
    "    return temp_embedding_1d\n",
    "\n",
    "\n",
    "def cosine_sim(embedding_1, embedding_2):\n",
    "    embedding_1 = get_normalise_embedding(embedding_1)\n",
    "    embedding_2 = get_normalise_embedding(embedding_2)\n",
    "    sim_sum = 0\n",
    "    for e_1, e_2 in zip(embedding_1, embedding_2):\n",
    "        sim_sum += (e_1*e_2)\n",
    "    return sim_sum\n",
    "def norm_ed_cosine_sim(embedding_1, embedding_2):\n",
    "    sim_sum = 0\n",
    "    for e_1, e_2 in zip(embedding_1, embedding_2):\n",
    "        sim_sum += (e_1*e_2)\n",
    "    return sim_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity -- Embedding Model\n",
    "\n",
    "def generic_sent_cos_sim(model_emb_func, t1, t2, additional_nesting = False):\n",
    "    if additional_nesting:\n",
    "        return cosine_sim(model_emb_func(t1)[0], model_emb_func(t2)[0])    \n",
    "    return cosine_sim(model_emb_func(t1), model_emb_func(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Semantic Segmentation Function PREPARATION FUNCTIONS\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def window(seq, n=3):\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def climb(co_score_list, list_index, mode = \"l\"):\n",
    "    res_score = 0\n",
    "    if mode == \"l\":\n",
    "        while (list_index >= 0):\n",
    "            if co_score_list[list_index] > res_score:\n",
    "                res_score = co_score_list[list_index]\n",
    "                list_index -= 1\n",
    "            else:\n",
    "                break\n",
    "        return res_score\n",
    "    else:\n",
    "        list_len = len(co_score_list)\n",
    "        while (list_index < list_len):\n",
    "            if co_score_list[list_index] > res_score:\n",
    "                res_score = co_score_list[list_index]\n",
    "                list_index += 1\n",
    "            else:\n",
    "                break\n",
    "        return res_score\n",
    "    \n",
    "def get_depth_score_list(co_score_list):\n",
    "    res_depth_score_list = []\n",
    "    co_score_len = len(co_score_list)\n",
    "    for i in range(co_score_len):\n",
    "        i_co_score = co_score_list[i]\n",
    "        l_peak = climb(co_score_list, i, \"l\")\n",
    "        r_peak = climb(co_score_list, i, \"r\")\n",
    "        i_depth_score = 0.5 * (l_peak + r_peak - (2*i_co_score))\n",
    "        res_depth_score_list.append(i_depth_score)\n",
    "    return np.array(res_depth_score_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import argrelmax\n",
    "\n",
    "def get_local_maxima(depth_scores, order=1):\n",
    "    maxima_ids = argrelmax(depth_scores, order=order)[0]\n",
    "    filtered_scores = np.zeros(len(depth_scores))\n",
    "    filtered_scores[maxima_ids] = depth_scores[maxima_ids]\n",
    "    return filtered_scores\n",
    "\n",
    "def compute_threshold(scores): ## maybe can make this more picky, by making threshold higher, like (np.std(s) / 3) or /4 or more instead?\n",
    "    s = scores[np.nonzero(scores)]\n",
    "    threshold = np.mean(s) - (np.std(s) / 2)\n",
    "    # threshold = np.mean(s) - (np.std(s))\n",
    "    return threshold\n",
    "\n",
    "def get_threshold_segments(scores, threshold=0.1):\n",
    "    segment_ids = np.where(scores >= threshold)[0]\n",
    "    return segment_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def primitively_naive_tokeniser(text):\n",
    "    toks_list = text.split(\" \")\n",
    "    return toks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Semantic Segmentation Function Portions\n",
    "\n",
    "WINDOW_SIZE = 3\n",
    "\n",
    "def sentence_to_sliding_window(sentence_s):\n",
    "    sentence_words_toks = primitively_naive_tokeniser(sentence_s)\n",
    "    window_size_split = list(window(sentence_words_toks, WINDOW_SIZE))\n",
    "    window_splited_texts = [' '.join([window_toks for window_toks in each_window]) for each_window in window_size_split]\n",
    "    return window_splited_texts\n",
    "\n",
    "def coherence_score_list_from_embedding_list(window_splited_embedding_list):\n",
    "    coherence_scores_list = [cosine_sim(pair[0], pair[1]) for pair in zip(window_splited_embedding_list[:-1], window_splited_embedding_list[1:])]\n",
    "    return coherence_scores_list\n",
    "\n",
    "def plot_data_points(vary_data, thres = -1):\n",
    "    plt.plot(vary_data)\n",
    "    if (thres == -1):\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.plot([thres for i in range(len(vary_data))])\n",
    "        plt.show()\n",
    "\n",
    "def filtered_indexes_list_to_splitted_segments_by_semantics(original_sent, filtered_indexes_list):\n",
    "    sentence_words_toks = primitively_naive_tokeniser(original_sent)\n",
    "    segment_key_breaks = get_threshold_segments(filtered_indexes_list, compute_threshold(filtered_indexes_list))\n",
    "    segment_demark = [0] + [(ids + (WINDOW_SIZE-1)) for ids in segment_key_breaks] + [len(sentence_words_toks)]\n",
    "    segment_demark_intervals = list(zip(segment_demark[:-1], segment_demark[1:]))\n",
    "    resultant_segments_after_split_by_interval = [\" \".join(sentence_words_toks[interval_points[0]:interval_points[1]]) for interval_points in segment_demark_intervals]\n",
    "    return resultant_segments_after_split_by_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Semantic Segmentation Function\n",
    "\n",
    "def semantic_segmentation_function(embedding_model_function, sentence_text, intermediate_status = False, graph_status = False):\n",
    "    windowed_parts = sentence_to_sliding_window(sentence_text)\n",
    "    if intermediate_status:\n",
    "        print(f\"windowed_parts: {windowed_parts}\")\n",
    "    \n",
    "    # if ensure \"embedding_model_function\" accept only 1 string and return 1d array/tensor then can use the below code, current should still work!!, as long as return 1d array for single string!!\n",
    "    # embedding_list = [embedding_model_function(windowed_part) for windowed_part in windowed_parts]\n",
    "    \n",
    "    ## if list of input strings can produce 2d array/tensor automatically, then can just use below one!!, only 1 time embed bunch at once!!\n",
    "    embedding_list = embedding_model_function(windowed_parts)\n",
    "    if intermediate_status:\n",
    "        print(f\"embedding_list: {embedding_list}\")\n",
    "    \"\"\"\n",
    "    if graph_status:\n",
    "        print(\"Embedding List Plot\") # bad! like no use\n",
    "        plot_data_points(embedding_list) # bad! like no use\n",
    "    \"\"\"\n",
    "    \n",
    "    windowed_parts_coherence_score_list = coherence_score_list_from_embedding_list(embedding_list)\n",
    "    if intermediate_status:\n",
    "        print(f\"windowed_parts_coherence_score_list: {windowed_parts_coherence_score_list}\")\n",
    "    if graph_status:\n",
    "        print(\"Coherence Score Plot:\")\n",
    "        plot_data_points(windowed_parts_coherence_score_list)\n",
    "    \n",
    "    windowed_parts_depth_score_list = get_depth_score_list(windowed_parts_coherence_score_list)\n",
    "    if intermediate_status:\n",
    "        print(f\"windowed_parts_depth_score_list: {windowed_parts_depth_score_list}\")\n",
    "    if graph_status:\n",
    "        print(\"Depth Score Plot:\")\n",
    "        plot_data_points(windowed_parts_depth_score_list)\n",
    "    \n",
    "    windowed_parts_filtered_depth_score_list = get_local_maxima(windowed_parts_depth_score_list)\n",
    "    if intermediate_status:\n",
    "        print(f\"windowed_parts_filtered_depth_score_list: {windowed_parts_filtered_depth_score_list}\")\n",
    "    if graph_status:\n",
    "        print(\"Filtered Depth Score Plot:\")\n",
    "        plot_data_points(windowed_parts_filtered_depth_score_list)\n",
    "    \n",
    "    filtered_threshold = compute_threshold(windowed_parts_filtered_depth_score_list)\n",
    "    if intermediate_status:\n",
    "        print(f\"filtered_threshold: {filtered_threshold}\")\n",
    "    if graph_status:\n",
    "        print(\"Filtered Depth Score With Threshold Line Plot:\")\n",
    "        plot_data_points(windowed_parts_filtered_depth_score_list, filtered_threshold)\n",
    "\n",
    "    #sentences_tokenised = primitively_naive_tokeniser(sentences)\n",
    "    #sentences_topics_splitted = filtered_indexes_list_to_splitted_sent(sentences_tokenised, windowed_sentences_filtered_depth_score_v1_list)\n",
    "    sentences_topics_splitted = filtered_indexes_list_to_splitted_segments_by_semantics(sentence_text, windowed_parts_filtered_depth_score_list)\n",
    "    return sentences_topics_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lock Model\n",
    "def lock_semantic_segmentation_function(embedding_model_function):\n",
    "    def lockED_semantic_segmentation_function(sentence_text, intermediate_status = False, graph_status = False): # all these default params need to have because the locked function can have the option to leave the args blank for them to let it be default!\n",
    "        return semantic_segmentation_function(embedding_model_function=embedding_model_function, sentence_text=sentence_text, intermediate_status=intermediate_status, graph_status=graph_status)\n",
    "    return lockED_semantic_segmentation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Similarity Comparison Function (comparison tuples in a list for comparison!)\n",
    "\n",
    "def generic_similarity_comparison_function(embedding_model_function, comparison_tuple_in_list, sort_output = 0):\n",
    "    res_dict = {}\n",
    "    for comp_items in comparison_tuple_in_list:\n",
    "        # possible alternative is below, so that if embedding model only accept one string and return 1d array/tensor then works!!\n",
    "        # comp_emb = [embedding_model_function(comp_items[0]), embedding_model_function(comp_items[1])]\n",
    "        comp_emb = embedding_model_function([comp_items[0], comp_items[1]]) # or just list(comp_items)\n",
    "        cos_sim = cosine_sim(comp_emb[0], comp_emb[1])\n",
    "        res_dict[comp_items] = cos_sim\n",
    "        \n",
    "    # sort by -1 is descending, 0 is no sort, 1 is ascending!\n",
    "    # default is no sort, 0\n",
    "    if sort_output == -1:\n",
    "        res_dict = {comp:comp_score for comp, comp_score in sorted(res_dict.items(), key = lambda dict_item: dict_item[1], reverse=True)}\n",
    "    if sort_output == 1:\n",
    "        res_dict = dict(sorted(res_dict.items(), key = lambda dict_item: dict_item[1], reverse=False))\n",
    "    return res_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial does not allow arguments to be filled with keywords, need strictly positional so prefer not\n",
    "\n",
    "## Error is like:\n",
    "# generic_similarity_comparison_locked_model_MiniLM_L6_v2 = lock_generic_similarity_comparison_function(get_sentence_embedding_MiniLM_L6_v2)\n",
    "# generic_similarity_comparison_locked_model_MiniLM_L6_v2([(\"hi there\", \"the world is bad\"), (\"i like people\", \"people love me\"), (\"the world is green\", \"the ocean is blue\")], sort_output=1)\n",
    "\n",
    "## Fix is need to specific keyword or change embedding callbaack function position and all, by keyword is like:\n",
    "# generic_similarity_comparison_locked_model_MiniLM_L6_v2 = lock_generic_similarity_comparison_function(get_sentence_embedding_MiniLM_L6_v2)\n",
    "# generic_similarity_comparison_locked_model_MiniLM_L6_v2(comparison_tuple_in_list=[(\"hi there\", \"the world is bad\"), (\"i like people\", \"people love me\"), (\"the world is green\", \"the ocean is blue\")], sort_output=1)\n",
    "## see the \"comparison_tuple_in_list=\" specified, for example! ^\n",
    "\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "\n",
    "def lock_generic_similarity_comparison_function(embedding_model_function):\n",
    "    return partial(generic_similarity_comparison_function, embedding_model_function=embedding_model_function)\n",
    "\"\"\"\n",
    "\n",
    "## instead of def new function, lambda approach!\n",
    "\n",
    "def lock_generic_similarity_comparison_function(embedding_model_function):\n",
    "    return lambda comparison_tuple_in_list, sort_output = 0: generic_similarity_comparison_function(embedding_model_function=embedding_model_function, comparison_tuple_in_list=comparison_tuple_in_list, sort_output=sort_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE Category Similarity Comparison Function (compare to each string in a list!)\n",
    "def single_category_similarity_comparison_function(embedding_model_function, category_single, texts, sort_output = 0):\n",
    "    if type(texts) != list:\n",
    "        texts = [texts]\n",
    "    compiled_tuple_comparison_list = [(text, category) for text, category in zip(texts, [category_single for i in range(len(texts))])]\n",
    "    comparison_result_dict = generic_similarity_comparison_function(embedding_model_function=embedding_model_function, comparison_tuple_in_list=compiled_tuple_comparison_list, sort_output=sort_output)\n",
    "    return comparison_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_single_category_similarity_comparison_function(embedding_model_function):\n",
    "    def lockED_single_category_similarity_comparison_function(category_single, texts, sort_output=0): ## sort_output=0 is needed since it can be left blank when called from locked model!\n",
    "        return single_category_similarity_comparison_function(embedding_model_function=embedding_model_function, category_single=category_single, texts=texts, sort_output=sort_output)\n",
    "    return lockED_single_category_similarity_comparison_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories Similarity Comparison Function (compare to each string in a list!)\n",
    "\n",
    "# sort by -1 is descending, 0 is no sort, 1 is ascending!\n",
    "# default is no sort, 0\n",
    "    \n",
    "def categories_similarity_comparison_function(embedding_model_function, categories, texts, sort_output = 0):\n",
    "    if type(categories) != list:\n",
    "        categories = [categories]\n",
    "    categories_comparison_result_dict = {}\n",
    "    for category in categories:\n",
    "        categories_comparison_result_dict[category] = single_category_similarity_comparison_function(embedding_model_function=embedding_model_function, category_single=category, texts=texts, sort_output=sort_output)\n",
    "    return categories_comparison_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from functools import partial\n",
    "\n",
    "def lock_categories_similarity_comparison_function(embedding_model_function):\n",
    "    return partial(categories_similarity_comparison_function, embedding_model_function=embedding_model_function)\n",
    "\"\"\"\n",
    "\n",
    "# lambda approach somewhat!, no keyword in the lambda now, just based off positional cos it can! but the sort_output=0 is a must, so that when call the locked function, if leave blank for it, wont error!\n",
    "def lock_categories_similarity_comparison_function(embedding_model_function):\n",
    "    return lambda categories, texts, sort_output=0: categories_similarity_comparison_function(embedding_model_function, categories, texts, sort_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories Similarity Result Display\n",
    "\n",
    "# Very specific use case only for \"single_category_similarity_comparison_function\" which returns a dict of compare_key and result_value\n",
    "# Not usable on \"categories_similarity_comparison_function\" since this returns will return dict of dict!\n",
    "\n",
    "def category_similarity_result_display(category_result_dict, sort_display = 0):\n",
    "    print(f\"Category: {list(category_result_dict.keys())[0][1]}\") ## trashy clusterfuck\n",
    "    print(\"Similarity Level:\")\n",
    "    if sort_display == -1:\n",
    "        for comparison_items_tuple, comparison_result in (sorted(category_result_dict.items(), key= lambda dict_item: dict_item[1], reverse=True)):\n",
    "            print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")\n",
    "    elif sort_display == 1:\n",
    "        for comparison_items_tuple, comparison_result in (sorted(category_result_dict.items(), key= lambda dict_item: dict_item[1], reverse=True)):\n",
    "            print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")\n",
    "    else:\n",
    "        for comparison_items_tuple, comparison_result in category_result_dict.items():\n",
    "            print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories Similarity Result Display\n",
    "def categories_similarity_result_display(categories_result_dict, sort_display = 0):\n",
    "    for category, category_similarity_results_dict in categories_result_dict.items():\n",
    "        print(f\"Category: {category}\")\n",
    "        print(\"Similarity Level:\")\n",
    "        if sort_display == -1:\n",
    "            for comparison_items_tuple, comparison_result in (sorted(category_similarity_results_dict.items(), key= lambda dict_item: dict_item[1], reverse=True)):\n",
    "                print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")\n",
    "        elif sort_display == 1:\n",
    "            for comparison_items_tuple, comparison_result in (sorted(category_similarity_results_dict.items(), key= lambda dict_item: dict_item[1], reverse=True)):\n",
    "                print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")\n",
    "        else:\n",
    "            for comparison_items_tuple, comparison_result in category_similarity_results_dict.items():\n",
    "                print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categories with sub-categories is in format of dictionary where general_category-key:sub-\"categories\"_in_list(actually more like \"synonyms\" of the general categories)-value\n",
    "## returns a dictionary of general_category-key:{sub-\"category\"(general category \"synonyms\")-key:{(xyz_comparison, sub-\"category\"/\"synonym\"):cosine_similarity}}\n",
    "\n",
    "def categories_wsub_similarity_comparison_function(embedding_model_function, categories_wsub_dict, texts, sort_output=0):\n",
    "    categories_wsub_result_dict = {}\n",
    "    for big_general_category, sub_categories in categories_wsub_dict.items():\n",
    "        categories_wsub_result_dict[big_general_category] = categories_similarity_comparison_function(embedding_model_function=embedding_model_function, categories=([big_general_category]+sub_categories), texts=texts, sort_output=sort_output)\n",
    "    return categories_wsub_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_categories_wsub_similarity_comparison_function(embedding_model_function):\n",
    "    return lambda categories_wsub_dict, texts, sort_output=0: categories_wsub_similarity_comparison_function(embedding_model_function=embedding_model_function, categories_wsub_dict=categories_wsub_dict, texts=texts, sort_output=sort_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_wsub_similarity_result_display(categories_wsub_result_dict, sort_display = 0):\n",
    "    for big_general_category, big_wsub_categories_result in categories_wsub_result_dict.items():\n",
    "        print(f\"General Category: {big_general_category}\")\n",
    "        categories_similarity_result_display(big_wsub_categories_result, sort_display=sort_display)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top xxx and Limit yyy, display function different mainly\n",
    "\n",
    "# Prep\n",
    "def categories_similarity_result_display_top_limit(categories_result_dict, top_many = 5, limit_value = 0.5):\n",
    "    for category, category_similarity_results_dict in categories_result_dict.items():\n",
    "        print(f\"Sub-Categories: {category}\")\n",
    "        print(f\"Similarity Level Of Top {top_many} (Limit={limit_value}):\")\n",
    "        num_count = 0\n",
    "        #if sort_display == -1:\n",
    "        for comparison_items_tuple, comparison_result in (sorted(category_similarity_results_dict.items(), key= lambda dict_item: dict_item[1], reverse=True)):\n",
    "            if num_count == top_many or comparison_result < limit_value:\n",
    "                break\n",
    "            print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")\n",
    "            num_count += 1\n",
    "        \"\"\"\n",
    "        elif sort_display == 1:\n",
    "            for comparison_items_tuple, comparison_result in (sorted(category_similarity_results_dict.items(), key= lambda dict_item: dict_item[1], reverse=True)):\n",
    "                if num_count == top_many or comparison_result < limit_value:\n",
    "                    break\n",
    "                print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")\n",
    "        else:\n",
    "            for comparison_items_tuple, comparison_result in category_similarity_results_dict.items():\n",
    "                if num_count == top_many or comparison_result < limit_value:\n",
    "                    break\n",
    "                print(f\"{comparison_items_tuple[0]:30.30} /-/ {comparison_items_tuple[1]:30.30} : {comparison_result:.5}\")\n",
    "        \"\"\"\n",
    "# Actual using function\n",
    "def categories_wsub_similarity_result_display_top_limit(categories_wsub_result_dict, top_many = 5, limit_value = 0.5):\n",
    "    for big_general_category, big_wsub_categories_result in categories_wsub_result_dict.items():\n",
    "        print(f\"General Category: {big_general_category}\")\n",
    "        general_category_subcats = tuple(big_wsub_categories_result.keys())\n",
    "        subcat_combined_dicts = {}\n",
    "        for subcat_dicts in big_wsub_categories_result.values():\n",
    "            subcat_combined_dicts = subcat_combined_dicts | subcat_dicts\n",
    "        #sorted_subcat_combined_dicts = {comp_tuple:comp_res for comp_tuple, comp_res in sorted(subcat_combined_dicts.items(), key = lambda dict_item: dict_item[1], reverse=True)}\n",
    "        categories_similarity_result_display_top_limit({general_category_subcats: subcat_combined_dicts}, top_many=top_many, limit_value=limit_value)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_wsub_similarity_comparison_resort_function(categories_wsub_similarity_comparison_result_dict, get_inner_list = False, sort_within_cat=0, top_many_wsub = 3, limit_value = 0.5):\n",
    "    resorted_categories_wsub_similarity_comparison_dict = {}\n",
    "    if limit_value < 0:\n",
    "        if sort_within_cat == -1:\n",
    "            limit_value = 0\n",
    "        if sort_within_cat == 0:\n",
    "            limit_value = None\n",
    "        if sort_within_cat == 1:\n",
    "            limit_value = 1\n",
    "    \n",
    "    if get_inner_list:\n",
    "        for category, sub_syn_cat_dict in categories_wsub_similarity_comparison_result_dict.items():\n",
    "            resorted_categories_wsub_similarity_comparison_dict[category] = []\n",
    "            for sub_syn_cat_text_tuple_pred_dict in sub_syn_cat_dict.values():\n",
    "                for sub_syn_cat_text_tuple, pred in sub_syn_cat_text_tuple_pred_dict.items():\n",
    "                    if sort_within_cat == 0 or (sort_within_cat == -1 and pred >= limit_value) or (sort_within_cat == 1 and pred <= limit_value):    \n",
    "                        resorted_categories_wsub_similarity_comparison_dict[category].append((sub_syn_cat_text_tuple, pred))\n",
    "        # sorting below is within a category itself\n",
    "        for category, comparison_tuple_pred_tuple in resorted_categories_wsub_similarity_comparison_dict.items():\n",
    "            if sort_within_cat == -1:\n",
    "                resorted_categories_wsub_similarity_comparison_dict[category] = list(sorted(comparison_tuple_pred_tuple, key= lambda tuple_item: tuple_item[1], reverse=True))[:top_many_wsub]\n",
    "            if sort_within_cat == 0:\n",
    "                resorted_categories_wsub_similarity_comparison_dict[category] = list(comparison_tuple_pred_tuple)[:top_many_wsub]\n",
    "            if sort_within_cat == 1:\n",
    "                resorted_categories_wsub_similarity_comparison_dict[category] = list(sorted(comparison_tuple_pred_tuple, key= lambda tuple_item: tuple_item[1], reverse=False))[:top_many_wsub]\n",
    "        return resorted_categories_wsub_similarity_comparison_dict\n",
    "            \n",
    "    else:    \n",
    "        for category, sub_syn_cat_dict in categories_wsub_similarity_comparison_result_dict.items():\n",
    "            resorted_categories_wsub_similarity_comparison_dict[category] = {}\n",
    "            for sub_syn_cat_text_tuple_pred_dict in sub_syn_cat_dict.values():\n",
    "                for sub_syn_cat_text_tuple, pred in sub_syn_cat_text_tuple_pred_dict.items():\n",
    "                    if sort_within_cat == 0 or (sort_within_cat == -1 and pred >= limit_value) or (sort_within_cat == 1 and pred <= limit_value):    \n",
    "                        resorted_categories_wsub_similarity_comparison_dict[category][sub_syn_cat_text_tuple] = pred\n",
    "        # sorting below is within a category itself\n",
    "        for category, comparison_tuple_pred_dict in resorted_categories_wsub_similarity_comparison_dict.items():\n",
    "            if sort_within_cat == -1:\n",
    "                resorted_categories_wsub_similarity_comparison_dict[category] = dict(list(sorted(comparison_tuple_pred_dict.items(), key= lambda dict_item: dict_item[1], reverse=True))[:top_many_wsub])\n",
    "            if sort_within_cat == 0:\n",
    "                resorted_categories_wsub_similarity_comparison_dict[category] = dict(list(comparison_tuple_pred_dict.items())[:top_many_wsub])\n",
    "            if sort_within_cat == 1:\n",
    "                resorted_categories_wsub_similarity_comparison_dict[category] = dict(list(sorted(comparison_tuple_pred_dict.items(), key= lambda dict_item: dict_item[1], reverse=False))[:top_many_wsub])\n",
    "    return resorted_categories_wsub_similarity_comparison_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No Sorting Order, all made for the cleaning function, but the code is there but the argument is removed and relevant code portion is commented out, after all, for no sort, how to determine top xxx category, the top or bottom!!\n",
    "\n",
    "def categories_wsub_similarity_comparison_resort_cleaning_function(resorted_categories_wsub_similarity_comparison_dict, get_inner_list = False, get_list = False, top_many_cat = 3):\n",
    "    resultant_cleaned_list = []\n",
    "    if get_inner_list:\n",
    "        for category, comparison_tuple_pred_pair_tuple_list in resorted_categories_wsub_similarity_comparison_dict.items():\n",
    "            for comparison_tuple, pred in comparison_tuple_pred_pair_tuple_list:\n",
    "                resultant_cleaned_list.append((category, (comparison_tuple, pred)))\n",
    "    else:\n",
    "        for category, comparison_tuple_pred_pair_dict in resorted_categories_wsub_similarity_comparison_dict.items():\n",
    "            for comparison_tuple, pred in comparison_tuple_pred_pair_dict.items():\n",
    "                resultant_cleaned_list.append((category, (comparison_tuple, pred)))\n",
    "    \"\"\"\n",
    "    # sort_cats args gone!! since cleaning is for top many, so no point giving option here, just restrict to just most to least!!\n",
    "    if sort_cats == -1:\n",
    "        sorted_resultant_cleaned_list = list(sorted(resultant_cleaned_list, key=lambda list_element: list_element[1][1], reverse=True))\n",
    "    if sort_cats == 0:\n",
    "        sorted_resultant_cleaned_list = resultant_cleaned_list\n",
    "    if sort_cats == 1:\n",
    "        sorted_resultant_cleaned_list = list(sorted(resultant_cleaned_list, key=lambda list_element: list_element[1][1], reverse=False))\n",
    "    \"\"\"\n",
    "    sorted_resultant_cleaned_list = list(sorted(resultant_cleaned_list, key=lambda list_element: list_element[1][1], reverse=True))[:top_many_cat]\n",
    "    if get_list:\n",
    "        return sorted_resultant_cleaned_list\n",
    "    \"\"\"\n",
    "    # for this to work with getting back a dict, which is sorted correctly, the \"top_many_wsub\" argument in previous function need to be 1\n",
    "    ## if not, very wonky, since category as key means \"top_many_cat\" has to be <= number of category, else weird, and if \"top_many_wsub\" is not 1, the method below not so direct, need to ensure only add to dict once, and if category added then no more replacement!\n",
    "    ### a possible alternative but unpreferred, so just keep \"top_many_wsub\" at 1 if following into this function!!\n",
    "    \n",
    "    sorted_resultant_cleaned_list_dict = {}\n",
    "    for cat, tuple_pair in sorted_resultant_cleaned_list:\n",
    "        if cat not in sorted_resultant_cleaned_list_dict:\n",
    "            sorted_resultant_cleaned_list_dict[cat] = tuple_pair\n",
    "    return sorted_resultant_cleaned_list_dict\n",
    "    \"\"\"\n",
    "    return dict(sorted_resultant_cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_categories_wsub_similarity_comparison_resorted_result_display(cleaned_resorted_compare_result, get_list):\n",
    "    if get_list:\n",
    "        for label, comparison_tuple_pred_pair_tuple in cleaned_resorted_compare_result:\n",
    "            print(f\"Category: {label}\")\n",
    "            print(f\"{comparison_tuple_pred_pair_tuple[0][0]:30.30} /-/ {comparison_tuple_pred_pair_tuple[0][1]:30.30}: {comparison_tuple_pred_pair_tuple[1]:.5}\")\n",
    "            print()\n",
    "    else:\n",
    "        for label, comparison_tuple_pred_pair_tuple in cleaned_resorted_compare_result.items():\n",
    "            print(f\"Category: {label}\")\n",
    "            print(f\"{comparison_tuple_pred_pair_tuple[0][0]:30.30} /-/ {comparison_tuple_pred_pair_tuple[0][1]:30.30}: {comparison_tuple_pred_pair_tuple[1]:.5}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(classifier, candidate_labels, sequence_to_classify, multi_label = True):\n",
    "    result_dict = {}\n",
    "    classifier_results = classifier(sequence_to_classify, candidate_labels, multi_label=multi_label)\n",
    "    if type(classifier_results) != list:\n",
    "        classifier_results = [classifier_results]\n",
    "    for classifier_result in classifier_results:\n",
    "        result_dict[classifier_result[\"sequence\"]] = {label:label_prob for label,label_prob in zip(classifier_result[\"labels\"], classifier_result[\"scores\"])}\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_classify_sentence(classifier):\n",
    "    return lambda candidate_labels, sequence_to_classify, multi_label = True: classify_sentence(classifier=classifier, candidate_labels=candidate_labels, sequence_to_classify=sequence_to_classify, multi_label=multi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_classification_function(classification_model_function, categories_candidate_labels, texts, multi_label = True, sort_output = 0):\n",
    "    classification_results = classify_sentence(classifier=classification_model_function, candidate_labels=categories_candidate_labels, sequence_to_classify=texts, multi_label=multi_label)\n",
    "    final_classified_dict = {}\n",
    "    if sort_output == -1:\n",
    "        final_classified_dict = classification_results\n",
    "        return final_classified_dict\n",
    "    if sort_output == 0:\n",
    "        for seq in texts:\n",
    "            final_classified_dict[seq] = {label:classification_results[seq][label] for label in categories_candidate_labels}\n",
    "        return final_classified_dict\n",
    "    if sort_output == 1:\n",
    "        for seq in texts:\n",
    "            pre_sort = {label:classification_results[seq][label] for label in categories_candidate_labels}\n",
    "            final_classified_dict[seq] = {label:label_pred for label, label_pred in sorted(pre_sort.items(), key = lambda dict_item: dict_item[1])}\n",
    "        return final_classified_dict\n",
    "\n",
    "## Resort Format Function\n",
    "def categories_classification_additional_resort_function(seq_classified_dictionary, categories_candidate_labels, sort_output = 0, top_many = 5, limit_value = 0.5):\n",
    "    if limit_value < 0:\n",
    "        if sort_output == -1:\n",
    "            limit_value = 0\n",
    "        if sort_output == 0:\n",
    "            limit_value = None\n",
    "        if sort_output == 1:\n",
    "            limit_value = 1\n",
    "    resorted_classification_dict = {label:{} for label in categories_candidate_labels}\n",
    "    for seq, label_to_label_pred_dict in seq_classified_dictionary.items():\n",
    "        for label in categories_candidate_labels:\n",
    "            if sort_output == -1:\n",
    "                if label_to_label_pred_dict[label] >= limit_value:\n",
    "                    resorted_classification_dict[label][seq] = label_to_label_pred_dict[label]\n",
    "            if sort_output == 0:\n",
    "                # limit_value no meaning here since no sorting so no >= or <= to base off\n",
    "                resorted_classification_dict[label][seq] = label_to_label_pred_dict[label]\n",
    "            if sort_output == 1:\n",
    "                if label_to_label_pred_dict[label] <= limit_value:\n",
    "                    resorted_classification_dict[label][seq] = label_to_label_pred_dict[label]\n",
    "    if sort_output == -1:\n",
    "        for label in categories_candidate_labels:\n",
    "            resorted_classification_dict[label] = dict(sorted(resorted_classification_dict[label].items(), key = lambda dict_item: dict_item[1], reverse=True))\n",
    "    if sort_output == 0:\n",
    "        resorted_classification_dict = resorted_classification_dict\n",
    "    if sort_output == 1:\n",
    "        for label in categories_candidate_labels:\n",
    "            resorted_classification_dict[label] = dict(sorted(resorted_classification_dict[label].items(), key = lambda dict_item: dict_item[1], reverse=False))\n",
    "    \n",
    "    if top_many >= 0:\n",
    "        for label in categories_candidate_labels:\n",
    "            resorted_classification_dict[label] = dict(list(resorted_classification_dict[label].items())[:top_many])\n",
    "    return resorted_classification_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_categories_classification_function(classification_model_function):\n",
    "    return lambda categories_candidate_labels, texts, multi_label = True, sort_output = 0 : categories_classification_function(classification_model_function=classification_model_function, categories_candidate_labels=categories_candidate_labels, texts=texts, multi_label = multi_label, sort_output = sort_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_classification_resorted_result_display(classification_resorted_dictionary_result, sort_display = 0, top_many = 5, limit_value = 0.5):\n",
    "    if limit_value < 0:\n",
    "        if sort_display == -1:\n",
    "            limit_value = 0\n",
    "        if sort_display == 0:\n",
    "            limit_value = None\n",
    "        if sort_display == 1:\n",
    "            limit_value = 1\n",
    "    if top_many > 0:\n",
    "        for label, seq_pred_dict in classification_resorted_dictionary_result.items():\n",
    "            print(f\"Category: {label}\")\n",
    "            if sort_display == -1:\n",
    "                for seq, pred in dict(sorted(list(seq_pred_dict.items()), key=lambda list_dict_tuple: list_dict_tuple[1], reverse=True)[:top_many]).items():\n",
    "                    if pred >= limit_value:\n",
    "                        print(f\"{seq:65.65}: {pred:.5}\")\n",
    "            if sort_display == 0:\n",
    "                ## if no sorting, then top xxx and limit yyy does not make sense so not applicable here\n",
    "                for seq, pred in seq_pred_dict.items():\n",
    "                    print(f\"{seq:65.65}: {pred:.5}\")\n",
    "            if sort_display == 1:\n",
    "                for seq, pred in dict(sorted(list(seq_pred_dict.items()), key=lambda list_dict_tuple: list_dict_tuple[1], reverse=False)[:top_many]).items():\n",
    "                    if pred <= limit_value:\n",
    "                        print(f\"{seq:65.65}: {pred:.5}\")\n",
    "            print()\n",
    "    else:\n",
    "        for label, seq_pred_dict in classification_resorted_dictionary_result.items():\n",
    "            print(f\"Category: {label}\")\n",
    "            if sort_display == -1:\n",
    "                for seq, pred in dict(sorted(seq_pred_dict.items(), key=lambda list_dict_tuple: list_dict_tuple[1], reverse=True)).items():\n",
    "                    if pred >= limit_value:\n",
    "                        print(f\"{seq:65.65}: {pred:.5}\")\n",
    "            if sort_display == 0:\n",
    "                ## if no sorting, then top xxx and limit yyy does not make sense so not applicable here\n",
    "                for seq, pred in seq_pred_dict.items():\n",
    "                    print(f\"{seq:65.65}: {pred:.5}\")\n",
    "            if sort_display == 1:\n",
    "                for seq, pred in dict(sorted(seq_pred_dict.items(), key=lambda list_dict_tuple: list_dict_tuple[1], reverse=False)).items():\n",
    "                    if pred <= limit_value:\n",
    "                        print(f\"{seq:65.65}: {pred:.5}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_classification_additional_resort_cleaning_function(classification_resorted_dictionary_result, get_list = False, top_many_cat = 3, limit_value = 0.5):\n",
    "    #return dict(list(dict(sorted(list(classification_resorted_dictionary_result.items()), key=lambda tuple_value_dict: list(tuple_value_dict[1].values())[0], reverse=True)).items())[:top_many_cat])\n",
    "    cleaned_classification_resorted_dictionary_result = {}\n",
    "    for label, seq_pred_dict in classification_resorted_dictionary_result.items():\n",
    "        ## the \"if\" part and the \"for\" part is done so that if seq_pred_dict.items() is empty, then next(iter()) wont crash if solely use it!!\n",
    "        \"\"\"\n",
    "        if len(seq_pred_dict) > 0:\n",
    "            cleaned_classification_resorted_dictionary_result[label] = next(iter(seq_pred_dict.items()))\n",
    "        \"\"\"\n",
    "        for seq, pred in seq_pred_dict.items():\n",
    "            cleaned_classification_resorted_dictionary_result[label] = (seq, pred)\n",
    "        ### if label dont have any that fits limit_value restriction, then the label wont appear in the dict at the end!!, not in this version at least!!!\n",
    "    cleaned_classification_resorted_dictionary_result = dict(sorted(cleaned_classification_resorted_dictionary_result.items(), key=lambda dict_item: dict_item[1][1], reverse=True))\n",
    "    if get_list:\n",
    "        return list(cleaned_classification_resorted_dictionary_result.items())[:top_many_cat]\n",
    "    return dict(list(cleaned_classification_resorted_dictionary_result.items())[:top_many_cat])\n",
    "\n",
    "\n",
    "def cleaned_categories_classification_resorted_result_display(cleaned_classification_resorted_result, get_list):\n",
    "    if get_list:\n",
    "        for label, seq_pred_tuple in cleaned_classification_resorted_result:\n",
    "            print(f\"Category: {label}\")\n",
    "            print(f\"{seq_pred_tuple[0]:65.65}: {seq_pred_tuple[1]:.5}\")\n",
    "            print()\n",
    "    else:\n",
    "        for label, seq_pred_tuple in cleaned_classification_resorted_result.items():\n",
    "            print(f\"Category: {label}\")\n",
    "            print(f\"{seq_pred_tuple[0]:65.65}: {seq_pred_tuple[1]:.5}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Combined Function (Similarity Comparison)\n",
    "\n",
    "def split_and_compare(split_embed_function, compare_embed_function, categories, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_split = True , display_end = True, sort_display = 0):\n",
    "    splitted_sentence_text = semantic_segmentation_function(embedding_model_function=split_embed_function, sentence_text=sentence_text, intermediate_status=intermediate, graph_status=graph)\n",
    "    if display_split:\n",
    "        print(f\"Splitted texts: {splitted_sentence_text}\")\n",
    "    compare_result = categories_similarity_comparison_function(embedding_model_function=compare_embed_function, categories=categories, texts=splitted_sentence_text, sort_output=sort_compare)\n",
    "    if display_end:\n",
    "        categories_similarity_result_display(compare_result, sort_display=sort_display)\n",
    "    return splitted_sentence_text, compare_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lock split and compare overall combined function\n",
    "def lock_split_and_compare(split_embed_function, compare_embed_function):\n",
    "    return lambda categories, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_split = True, display_end = True, sort_display = 0: split_and_compare(split_embed_function=split_embed_function, compare_embed_function=compare_embed_function, categories=categories, sentence_text=sentence_text, intermediate=intermediate, graph=graph, sort_compare=sort_compare, display_split=display_split, display_end=display_end, sort_display=sort_display)\n",
    "    # SyntaxError: positional argument follows keyword argument\n",
    "    # return lambda categories, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_end = True, sort_display = 0: split_and_compare(split_embed_function=split_embed_function, compare_embed_function=compare_embed_function, categories, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_end = True, sort_display = 0)\n",
    "\n",
    "### OR !!\n",
    "\n",
    "def lock_split_and_compare(split_embed_function, compare_embed_function):\n",
    "    def lockED_split_and_compare(categories, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_split = True , display_end = True, sort_display = 0):\n",
    "        return split_and_compare(split_embed_function, compare_embed_function, categories, sentence_text, intermediate = intermediate, graph = graph, sort_compare = sort_compare, display_split = display_split , display_end = display_end, sort_display = sort_display)\n",
    "    return lockED_split_and_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_compare_wsub(split_embed_function, compare_embed_function, categories_wsub, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_split = True , display_end = True, sort_display = 0):\n",
    "    splitted_sentence_text = semantic_segmentation_function(embedding_model_function=split_embed_function, sentence_text=sentence_text, intermediate_status=intermediate, graph_status=graph)\n",
    "    if display_split:\n",
    "        print(f\"Splitted texts: {splitted_sentence_text}\")\n",
    "    compare_result = categories_wsub_similarity_comparison_function(embedding_model_function=compare_embed_function, categories_wsub_dict=categories_wsub, texts=splitted_sentence_text, sort_output=sort_compare)\n",
    "    if display_end:\n",
    "        categories_wsub_similarity_result_display(compare_result, sort_display=sort_display)\n",
    "    return splitted_sentence_text, compare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_split_and_compare_wsub(split_embed_function, compare_embed_function):\n",
    "    def lockED_split_and_compare_wsub(categories_wsub, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_split = True , display_end = True, sort_display = 0):\n",
    "        return split_and_compare_wsub(split_embed_function, compare_embed_function, categories_wsub=categories_wsub, sentence_text=sentence_text, intermediate = intermediate, graph = graph, sort_compare = sort_compare, display_split = display_split , display_end = display_end, sort_display = sort_display)\n",
    "    return lockED_split_and_compare_wsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_compare_wsub_top_limit(split_embed_function, compare_embed_function, categories_wsub, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_split = True , display_end = True, top_many = 5, limit_value = 0.5):\n",
    "    splitted_sentence_text = semantic_segmentation_function(embedding_model_function=split_embed_function, sentence_text=sentence_text, intermediate_status=intermediate, graph_status=graph)\n",
    "    if display_split:\n",
    "        print(f\"Splitted texts: {splitted_sentence_text}\")\n",
    "    compare_result = categories_wsub_similarity_comparison_function(embedding_model_function=compare_embed_function, categories_wsub_dict=categories_wsub, texts=splitted_sentence_text, sort_output=sort_compare)\n",
    "    if display_end:\n",
    "        categories_wsub_similarity_result_display_top_limit(compare_result, top_many=top_many, limit_value=limit_value)\n",
    "    return splitted_sentence_text, compare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_split_and_compare_wsub_top_limit(split_embed_function, compare_embed_function):\n",
    "    return lambda categories_wsub, sentence_text, intermediate = False, graph = False, sort_compare = 0, display_split = True , display_end = True, top_many = 5, limit_value = 0.5:split_and_compare_wsub_top_limit(split_embed_function=split_embed_function, compare_embed_function=compare_embed_function, categories_wsub=categories_wsub, sentence_text=sentence_text, intermediate = intermediate, graph = graph, sort_compare = sort_compare, display_split = display_split , display_end = display_end, top_many = top_many, limit_value = limit_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_compare_wsub_top_limit_cleaned(split_embed_function, compare_embed_function, categories_wsub, sentence_text, intermediate = False, graph = False, sort_compare = 0, get_inner_list = False, get_list = False, display_split = True , display_end = True, top_many_cat = 3, limit_value = 0.5, extra_clean_output=False):\n",
    "    splitted_sentence_text = semantic_segmentation_function(embedding_model_function=split_embed_function, sentence_text=sentence_text, intermediate_status=intermediate, graph_status=graph)\n",
    "    if display_split:\n",
    "        print(f\"Splitted texts: {splitted_sentence_text}\")\n",
    "    compare_result = categories_wsub_similarity_comparison_function(embedding_model_function=compare_embed_function, categories_wsub_dict=categories_wsub, texts=splitted_sentence_text, sort_output=sort_compare)\n",
    "    resorted_compare_result = categories_wsub_similarity_comparison_resort_function(categories_wsub_similarity_comparison_result_dict=compare_result, get_inner_list=get_inner_list, sort_within_cat=-1, top_many_wsub=1, limit_value=limit_value) ## sort_within_cat=-1, top_many_wsub=1 are both need!! for the purpose of this cleaning up function part!!\n",
    "    cleaned_resorted_compare_result = categories_wsub_similarity_comparison_resort_cleaning_function(resorted_categories_wsub_similarity_comparison_dict=resorted_compare_result, get_inner_list=get_inner_list, get_list=get_list, top_many_cat=top_many_cat)\n",
    "    if display_end:\n",
    "        cleaned_categories_wsub_similarity_comparison_resorted_result_display(cleaned_resorted_compare_result, get_list=get_list)\n",
    "    if extra_clean_output:\n",
    "        if get_list:\n",
    "            return splitted_sentence_text, [(category, compare_sim_tuple[1]) for category, compare_sim_tuple in cleaned_resorted_compare_result]\n",
    "        else:\n",
    "            return splitted_sentence_text, {category:compare_sim_tuple[1] for category, compare_sim_tuple in cleaned_resorted_compare_result.items()}\n",
    "    return splitted_sentence_text, cleaned_resorted_compare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_split_and_compare_wsub_top_limit_cleaned(split_embed_function, compare_embed_function):\n",
    "    return lambda categories_wsub, sentence_text, intermediate = False, graph = False, sort_compare = 0, get_inner_list = False, get_list = False, display_split = True , display_end = True, top_many_cat = 3, limit_value = 0.5, extra_clean_output=False: split_and_compare_wsub_top_limit_cleaned(split_embed_function=split_embed_function, compare_embed_function=compare_embed_function, categories_wsub=categories_wsub, sentence_text=sentence_text, intermediate = intermediate, graph = graph, sort_compare = sort_compare, get_inner_list = get_inner_list, get_list = get_list, display_split = display_split , display_end = display_end, top_many_cat = top_many_cat, limit_value = limit_value, extra_clean_output=extra_clean_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split and Classify, using Zero Shot Classification\n",
    "def split_and_classify(split_embed_function, classify_function, candidate_possible_labels, sentence_text, intermediate = False, graph = False, multi_label=True, sort_classify = 0, additional_resort = True, display_split = True , display_end = True, sort_display = 0):\n",
    "    splitted_sentence_text = semantic_segmentation_function(embedding_model_function=split_embed_function, sentence_text=sentence_text, intermediate_status=intermediate, graph_status=graph)\n",
    "    if display_split:\n",
    "        print(f\"Splitted texts: {splitted_sentence_text}\")\n",
    "    classification_result = categories_classification_function(classification_model_function=classify_function, categories_candidate_labels=candidate_possible_labels, texts=splitted_sentence_text, multi_label=multi_label, sort_output=sort_classify)\n",
    "    if additional_resort:\n",
    "        resorted_classification_result = categories_classification_additional_resort_function(seq_classified_dictionary=classification_result, categories_candidate_labels=candidate_possible_labels, sort_output=sort_classify, top_many=-1, limit_value=0)\n",
    "    if display_end:\n",
    "        display_usage_resorted_classification_result = categories_classification_additional_resort_function(seq_classified_dictionary=classification_result, categories_candidate_labels=candidate_possible_labels, sort_output=0, top_many=-1, limit_value=-1)\n",
    "        categories_classification_resorted_result_display(classification_resorted_dictionary_result=display_usage_resorted_classification_result, sort_display=sort_display, top_many=-1, limit_value=-1)\n",
    "    if additional_resort:\n",
    "        return splitted_sentence_text, resorted_classification_result\n",
    "    return splitted_sentence_text, classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_split_and_classify(split_embed_function, classify_function):\n",
    "    return lambda candidate_possible_labels, sentence_text, intermediate = False, graph = False, multi_label=True, sort_classify = 0, additional_resort = True, display_split = True , display_end = True, sort_display = 0: split_and_classify(split_embed_function=split_embed_function, classify_function=classify_function, candidate_possible_labels=candidate_possible_labels, sentence_text=sentence_text, intermediate = intermediate, graph = graph, multi_label=multi_label, sort_classify = sort_classify, additional_resort = additional_resort, display_split = display_split , display_end = display_end, sort_display = sort_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_classify_top_limit(split_embed_function, classify_function, candidate_possible_labels, sentence_text, intermediate = False, graph = False, multi_label=True, sort_classify = 0, additional_resort = True, display_split = True , display_end = True, sort_display = 0, top_many=5, limit_value=0.5):\n",
    "    splitted_sentence_text = semantic_segmentation_function(embedding_model_function=split_embed_function, sentence_text=sentence_text, intermediate_status=intermediate, graph_status=graph)\n",
    "    if display_split:\n",
    "        print(f\"Splitted texts: {splitted_sentence_text}\")\n",
    "    classification_result = categories_classification_function(classification_model_function=classify_function, categories_candidate_labels=candidate_possible_labels, texts=splitted_sentence_text, multi_label=multi_label, sort_output=sort_classify)\n",
    "    if additional_resort:\n",
    "        resorted_classification_result = categories_classification_additional_resort_function(seq_classified_dictionary=classification_result, categories_candidate_labels=candidate_possible_labels, sort_output=sort_classify, top_many=top_many, limit_value=limit_value)\n",
    "    if display_end:\n",
    "        display_usage_resorted_classification_result = categories_classification_additional_resort_function(seq_classified_dictionary=classification_result, categories_candidate_labels=candidate_possible_labels, sort_output=0, top_many=-1, limit_value=-1)\n",
    "        categories_classification_resorted_result_display(classification_resorted_dictionary_result=display_usage_resorted_classification_result, sort_display=sort_display, top_many=top_many, limit_value=limit_value)\n",
    "    if additional_resort:\n",
    "        return splitted_sentence_text, resorted_classification_result\n",
    "    return splitted_sentence_text, classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_split_and_classify_top_limit(split_embed_function, classify_function):\n",
    "    return lambda candidate_possible_labels, sentence_text, intermediate = False, graph = False, multi_label=True, sort_classify = 0, additional_resort = True, display_split = True , display_end = True, sort_display = 0, top_many=5, limit_value=0.5: split_and_classify_top_limit(split_embed_function=split_embed_function, classify_function=classify_function, candidate_possible_labels=candidate_possible_labels, sentence_text=sentence_text, intermediate = intermediate, graph = graph, multi_label=multi_label, sort_classify = sort_classify, additional_resort = additional_resort, display_split = display_split , display_end = display_end, sort_display = sort_display, top_many=top_many, limit_value=limit_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_classify_top_limit_cleaned(split_embed_function, classify_function, candidate_possible_labels, sentence_text, intermediate = False, graph = False, multi_label=True, get_list = False, display_split = True , display_end = True, top_many_cat=3, limit_value=0.5, extra_clean_output=False):\n",
    "    splitted_sentence_text = semantic_segmentation_function(embedding_model_function=split_embed_function, sentence_text=sentence_text, intermediate_status=intermediate, graph_status=graph)\n",
    "    if display_split:\n",
    "        print(f\"Splitted texts: {splitted_sentence_text}\")\n",
    "    classification_result = categories_classification_function(classification_model_function=classify_function, categories_candidate_labels=candidate_possible_labels, texts=splitted_sentence_text, multi_label=multi_label)\n",
    "    resorted_classification_result = categories_classification_additional_resort_function(seq_classified_dictionary=classification_result, categories_candidate_labels=candidate_possible_labels, sort_output=-1, top_many=1, limit_value=limit_value) # the sort_output = -1 and top_many = 1 is both impt!!!\n",
    "    cleaned_classification_resorted_result = categories_classification_additional_resort_cleaning_function(classification_resorted_dictionary_result=resorted_classification_result, get_list=get_list, top_many_cat=top_many_cat, limit_value=limit_value)\n",
    "\n",
    "    if display_end:\n",
    "        cleaned_categories_classification_resorted_result_display(cleaned_classification_resorted_result=cleaned_classification_resorted_result, get_list=get_list)\n",
    "    \n",
    "    if extra_clean_output:\n",
    "        if get_list:\n",
    "            return splitted_sentence_text, [(category_label, seq_pred_tuple[1]) for category_label, seq_pred_tuple in cleaned_classification_resorted_result]\n",
    "        else:\n",
    "            return splitted_sentence_text, {category_label:seq_pred_tuple[1] for category_label, seq_pred_tuple in cleaned_classification_resorted_result.items()}\n",
    "    return splitted_sentence_text, cleaned_classification_resorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_split_and_classify_top_limit_cleaned(split_embed_function, classify_function):\n",
    "    return lambda candidate_possible_labels, sentence_text, intermediate = False, graph = False, multi_label=True, get_list=False, display_split = True , display_end = True, top_many_cat=3, limit_value=0.5, extra_clean_output=False: split_and_classify_top_limit_cleaned(split_embed_function=split_embed_function, classify_function=classify_function, candidate_possible_labels=candidate_possible_labels, sentence_text=sentence_text, intermediate = intermediate, graph = graph, multi_label=multi_label, get_list=get_list, display_split = display_split , display_end = display_end, top_many_cat=top_many_cat, limit_value=limit_value, extra_clean_output=extra_clean_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT YET WITH ZERO SHOT CLASSIFIER FUNCTION!\n",
    "\n",
    "### maybe also rename the functions to be like xxx_emb, or smth easier then later on use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Functions (xxx_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \"\"\"\n",
    "    print(attention_mask.shape, attention_mask)\n",
    "    print(attention_mask.unsqueeze(-1).shape, attention_mask.unsqueeze(-1))\n",
    "    print(input_mask_expanded)\n",
    "    print(len(input_mask_expanded), token_embeddings.size(), input_mask_expanded.size())\n",
    "    \"\"\"\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\"\"\"\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\"\"\"\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "pt_transformers_L6_v2_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "pt_transformers_L6_v2_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def all_MiniLM_L6_v2_embedding(sentences):\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    encoded_input = pt_transformers_L6_v2_tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = pt_transformers_L6_v2_model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    #print(\"Sentence embeddings:\")\n",
    "    #print(sentence_embeddings)\n",
    "    '''\n",
    "    if len(sentence_embeddings) == 1:\n",
    "        return sentence_embeddings[0]\n",
    "    return sentence_embeddings\n",
    "    '''\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-transformers/all-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \"\"\"\n",
    "    print(attention_mask.shape, attention_mask)\n",
    "    print(attention_mask.unsqueeze(-1).shape, attention_mask.unsqueeze(-1))\n",
    "    print(input_mask_expanded)\n",
    "    print(len(input_mask_expanded), token_embeddings.size(), input_mask_expanded.size())\n",
    "    \"\"\"\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\"\"\"\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\"\"\"\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "pt_transformers_L12_v2_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "pt_transformers_L12_v2_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "def all_MiniLM_L12_v2_embedding(sentences):\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    encoded_input = pt_transformers_L12_v2_tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = pt_transformers_L12_v2_model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    #print(\"Sentence embeddings:\")\n",
    "    #print(sentence_embeddings)\n",
    "    '''\n",
    "    if len(sentence_embeddings) == 1:\n",
    "        return sentence_embeddings[0]\n",
    "    return sentence_embeddings\n",
    "    '''\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAAI/bge-large-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "#tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "#model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "tokenizer_bge = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "model_bge = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "model_bge.eval()\n",
    "\n",
    "def bge_large_v1_5_embedding(sentenceS): ## already normalised due to \"torch.nn.functional.normalize\" function\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer_bge(sentenceS, padding=True, truncation=True, return_tensors='pt')\n",
    "    # for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
    "    # encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model_bge(**encoded_input)\n",
    "        # Perform pooling. In this case, cls pooling.\n",
    "        sentenceS_embeddings = model_output[0][:, 0]\n",
    "    # normalize embeddings\n",
    "    sentenceS_embeddings = torch.nn.functional.normalize(sentenceS_embeddings, p=2, dim=1)\n",
    "    #print(\"SentenceS embeddings:\", sentenceS_embeddings)\n",
    "    '''\n",
    "    if len(sentenceS_embeddings) == 1:\n",
    "        return sentenceS_embeddings[0]\n",
    "    return sentenceS_embeddings ## if not input a list of sentences, then just one\n",
    "    '''\n",
    "    return sentenceS_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### facebook/bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "\n",
    "bart_large_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "bart_large_model = BartModel.from_pretrained('facebook/bart-large')\n",
    "def bart_cls_emb(sentences):\n",
    "    inputs = bart_large_tokenizer(sentences, return_tensors=\"pt\")\n",
    "    outputs = bart_large_model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    return last_hidden_states[:, 0]\n",
    "\n",
    "def bart_mean_emb(sentences):\n",
    "    inputs = bart_large_tokenizer(sentences, return_tensors=\"pt\")\n",
    "    outputs = bart_large_model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "\n",
    "    return torch.sum(last_hidden_states * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "def bart_pad_cls_emb(sentences):\n",
    "    inputs = bart_large_tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "    outputs = bart_large_model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    return last_hidden_states[:, 0]\n",
    "\n",
    "def bart_pad_mean_emb(sentences):\n",
    "    inputs = bart_large_tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "    outputs = bart_large_model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "\n",
    "    return torch.sum(last_hidden_states * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def bart_large_cls_embedding(sentences):\n",
    "    inputs = bart_large_tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "    outputs = bart_large_model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    return last_hidden_states[:, 0]\n",
    "\n",
    "def bart_large_mean_embedding(sentences):\n",
    "    inputs = bart_large_tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "    outputs = bart_large_model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "\n",
    "    return torch.sum(last_hidden_states * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google/canine-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CanineTokenizer, CanineModel\n",
    "\n",
    "Canine_model = CanineModel.from_pretrained('google/canine-c')\n",
    "Canine_tokenizer = CanineTokenizer.from_pretrained('google/canine-c')\n",
    "\n",
    "def g_canine_embedding(sentences_input):\n",
    "    #sentences_input = [\"Life is like a box of chocolates.\", \"You never know what you gonna get.\"]\n",
    "    encoding = Canine_tokenizer(sentences_input, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = Canine_model(**encoding) # forward pass\n",
    "    pooled_output = outputs.pooler_output\n",
    "    #print(pooled_output)\n",
    "    sequence_output = outputs.last_hidden_state\n",
    "    \n",
    "    \"\"\"\n",
    "    sentence_cls_emb = mean_pooling(outputs, encoding[\"attention_mask\"])\n",
    "    sentence_cls_emb = F.normalize(sentence_cls_emb, p=2, dim=1)\n",
    "\n",
    "    if len(sentence_cls_emb) == 1:\n",
    "        return sentence_cls_emb[0]\n",
    "    return sentence_cls_emb\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    if len(sequence_output) == 1:\n",
    "        return sequence_output[0][0]\n",
    "    return sequence_output[:, 0]\n",
    "    '''\n",
    "    return sequence_output[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mixedbread-ai/mxbai-embed-large-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IRASUser\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "mxbai_v1_dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "mxbai_v1_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=mxbai_v1_dimensions)\n",
    "\n",
    "def mxbai_large_v1_embedding(sentences):\n",
    "    # For retrieval you need to pass this prompt.\n",
    "    query = 'Represent this sentence for searching relevant passages: A man is eating a piece of bread'\n",
    "\n",
    "    docs = sentences\n",
    "\n",
    "    # 2. Encode\n",
    "    embeddings = mxbai_v1_model.encode(docs)\n",
    "\n",
    "    # Optional: Quantize the embeddings\n",
    "    #binary_embeddings = quantize_embeddings(embeddings, precision=\"ubinary\")\n",
    "\n",
    "    #similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "    #print('similarities:', similarities)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "'''\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "'''\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "all_mpnet_base_v2_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "all_mpnet_base_v2_model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def all_mpnet_base_v2_embedding(sentences):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = all_mpnet_base_v2_tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = all_mpnet_base_v2_model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-transformers/paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "paraphrase_MiniLM_L6_v2_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def paraphrase_MiniLM_L6_v2_embedding(sentences):\n",
    "    embeddings = paraphrase_MiniLM_L6_v2_model.encode(sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "paraphrase_multilingual_MiniLM_L12_v2_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "def paraphrase_multilingual_MiniLM_L12_v2_embedding(sentences):\n",
    "    embeddings = paraphrase_multilingual_MiniLM_L12_v2_model.encode(sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alibaba-NLP/gte-large-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires sentence_transformers>=2.7.0\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "gte_large_en_v1_5_model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "\n",
    "def gte_large_en_v1_5_embedding(sentences):\n",
    "    embeddings = gte_large_en_v1_5_model.encode(sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nomic-ai/nomic-embed-text-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\irasuser\\anaconda3\\lib\\site-packages (0.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "!pip install einops\n",
    "\n",
    "nomic_embed_text_v1_5_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "def nomic_embed_text_v1_5_embedding(sentences):\n",
    "    matryoshka_dim = 512\n",
    "    if type(sentences) != list:\n",
    "        sentences = [sentences]\n",
    "    sentences = [(\"search_document: \" + sentence) for sentence in sentences]\n",
    "    embeddings = nomic_embed_text_v1_5_model.encode(sentences, convert_to_tensor=True)\n",
    "    embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n",
    "    embeddings = embeddings[:, :matryoshka_dim]\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification Functions (xxx_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\"\"\"\n",
    "bart_mnli_classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\"\"\"\n",
    "\n",
    "bart_mnli_classifier_pipeline = pipeline(\"zero-shot-classification\",\n",
    "                    model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "\"\"\"\n",
    "def bart_mnli_classifier(*classifier_input, **keyword_args):\n",
    "    if \"multi_label\" in keyword_args:\n",
    "        return bart_mnli_classifier_pipeline(*classifier_input, multi_label=keyword_args[\"multi_label\"])\n",
    "    else:\n",
    "        return bart_mnli_classifier_pipeline(*classifier_input)\n",
    "\"\"\"\n",
    "\n",
    "## above works but this is just cleaner since i know only got multi_label argument being used here!!\n",
    "def bart_mnli_classifier(*classifier_input, multi_label = False):\n",
    "    return bart_mnli_classifier_pipeline(*classifier_input, multi_label=multi_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-encoder/nli-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nli_roberta_base_classifier_pipeline = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-roberta-base')\n",
    "\n",
    "\n",
    "def nli_roberta_base_classifier(*classifier_input, multi_label = False):\n",
    "    return nli_roberta_base_classifier_pipeline(*classifier_input, multi_label=multi_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[sentencepiece]\n",
    "from transformers import pipeline\n",
    "deberta_v3_base_mnli_fever_anli_classifier_pipeline = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
    "\n",
    "def deberta_v3_base_mnli_fever_anli_classifier(*classifier_input, multi_label=False):\n",
    "    return deberta_v3_base_mnli_fever_anli_classifier_pipeline(*classifier_input, multi_label=multi_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data and Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_functions_list = [\n",
    "    all_MiniLM_L6_v2_embedding,\n",
    "    all_MiniLM_L12_v2_embedding,\n",
    "    bge_large_v1_5_embedding,\n",
    "    bart_large_cls_embedding,\n",
    "    bart_large_mean_embedding,\n",
    "    g_canine_embedding,\n",
    "    mxbai_large_v1_embedding,\n",
    "    all_mpnet_base_v2_embedding,\n",
    "    paraphrase_MiniLM_L6_v2_embedding,\n",
    "    paraphrase_multilingual_MiniLM_L12_v2_embedding,\n",
    "    gte_large_en_v1_5_embedding,\n",
    "    nomic_embed_text_v1_5_embedding\n",
    "]\n",
    "\n",
    "classification_functions_list = [\n",
    "    bart_mnli_classifier,\n",
    "    nli_roberta_base_classifier,\n",
    "    deberta_v3_base_mnli_fever_anli_classifier\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_for_embedding = {\n",
    "\"Tax principle\": [\"\"],\n",
    "\"List of benefits-in-kind granted administrative concession or exempt from income tax\": [\"\"],\n",
    "\"Flexible benefits scheme\": [\"\"],\n",
    "\"Accommodation provided to an employee/director\": [\"\"],\n",
    "\"Furniture and fittings and related benefits\": [\"\"],\n",
    "\"Serviced apartment\": [\"\"],\n",
    "\"Hotel\": [\"\"],\n",
    "\"Housing allowance\": [\"\"],\n",
    "\"Home leave passage\": [\"\"],\n",
    "\"Cash payment in-lieu of home leave passage\": [\"\"],\n",
    "\"Passage provided for business purpose\": [\"\"],\n",
    "\"Passage provided when taking up employment and upon termination\": [\"\"],\n",
    "\"Award for passing exams\": [\"\"],\n",
    "\"Bursary\": [\"\"],\n",
    "\"Innovation/ Improvement\": [\"\"],\n",
    "\"Long service/ retirement\": [\"\"],\n",
    "\"Recognition of good service\": [\"\"],\n",
    "\"Recognition of work performance\": [\"\"],\n",
    "\"Referral\": [\"\"],\n",
    "\"Zero/ low MC\": [\"\"],\n",
    "\"Food, door gifts and lucky draw prizes\": [\"\"],\n",
    "\"Dinner and dance held overseas\": [\"\"],\n",
    "\"Interest benefits arising from interest-free or subsidised interest loan\": [\"\"],\n",
    "\"Interest benefits on loans to company directors\": [\"\"],\n",
    "\"Waiver of principal sum\": [\"\"],\n",
    "\"Car provided\": [\"\"],\n",
    "\"Commercial vehicle provided\": [\"\"],\n",
    "\"Car park charges\": [\"\"],\n",
    "\"ERP charges\": [\"\"],\n",
    "\"Mileage for business usage\": [\"\"],\n",
    "\"Taxes, repairs and maintenance expenses of employee's own vehicle\": [\"\"],\n",
    "\"Subsidy for a child in childcare center\": [\"\"],\n",
    "\"Subsidy for a child in student care\": [\"\"],\n",
    "\"Employer's contributions relating to employment in Singapore\": [\"\"],\n",
    "\"Contributions made from 1 Jan 2004 relating to employment outside Singapore\": [\"\"],\n",
    "\"Contributions relating to director's fees\": [\"\"],\n",
    "\"Festive occasions\": [\"\"],\n",
    "\"Special occasions\": [\"\"],\n",
    "\"Bereavement\": [\"\"],\n",
    "\"Insurance premium\": [\"\"],\n",
    "\"Group medical insurance\": [\"\"],\n",
    "\"Group insurance policy\": [\"\"],\n",
    "\"Travel insurance covering the period of business travel\": [\"\"],\n",
    "\"Work injury compensation\": [\"\"],\n",
    "\"Death gratuities/ Injuries or disability payments/ Workmen compensation\": [\"\"],\n",
    "\"Gratuity for completing number of years of service\": [\"\"],\n",
    "\"Payment to induce a person to join the company\": [\"\"],\n",
    "\"Retrenchment payment to compensate loss of employment\": [\"\"],\n",
    "\"Retirement benefits\": [\"\"],\n",
    "\"Payment made to employee for entering into covenant\": [\"\"],\n",
    "\"Salary in lieu of notice/notice pay\": [\"\"],\n",
    "\"Fixed monthly meal allowance\": [\"\"],\n",
    "\"Working overtime - allowance paid or reimbursement made\": [\"\"],\n",
    "\"Free or subsidised food and drinks\": [\"\"],\n",
    "\"Reimbursement for employees and dependants\": [\"\"],\n",
    "\"Medical benefit based on gender or age\": [\"\"],\n",
    "\"Medical insurance\": [\"\"],\n",
    "\"Transport expenses to see doctor\": [\"\"],\n",
    "\"Absentee payroll under Skills Redevelopment programme\": [\"\"],\n",
    "\"Conditional payments made in advance\": [\"\"],\n",
    "\"Encashment of unutilised leave\": [\"\"],\n",
    "\"Inflation bonus\": [\"\"],\n",
    "\"Laundry allowance\": [\"\"],\n",
    "\"Maternity leave benefit\": [\"\"],\n",
    "\"NSman pay\": [\"\"],\n",
    "\"Skills Development Levy (SDL)\": [\"\"],\n",
    "\"Contributions made by employer to employee's Supplementary Retirement Scheme (SRS) account\": [\"\"],\n",
    "\"Relocation allowance\": [\"\"],\n",
    "\"Contributions made by employer to any pension/provident fund outside Singapore\": [\"\"],\n",
    "\"Employment Assistance Payment (EAP)\": [\"\"],\n",
    "\"Overseas holiday trips\": [\"\"],\n",
    "\"Holiday reimbursement\": [\"\"],\n",
    "\"Overtime allowance\": [\"\"],\n",
    "\"Overtime claims\": [\"\"],\n",
    "\"Per diem allowance\": [\"\"],\n",
    "\"Per diem reimbursement\": [\"\"],\n",
    "\"Combination of allowance and reimbursement\": [\"\"],\n",
    "\"Parking fees at the airport\": [\"\"],\n",
    "\"Travel insurance premium\": [\"\"],\n",
    "\"Travel between home and airport\": [\"\"],\n",
    "\"Payment for warm clothing\": [\"\"],\n",
    "\"Payment for luggage\": [\"\"],\n",
    "\"Facilities owned by employer\": [\"\"],\n",
    "\"Reimbursement for renting chalet\": [\"\"],\n",
    "\"Corporate passes to places of interest\": [\"\"],\n",
    "\"Staff discount offered by employer or its related entities\": [\"\"],\n",
    "\"Staff discount extended to employee's family members, relatives and friends\": [\"\"],\n",
    "\"Employee Share Option (ESOP)\": [\"\"],\n",
    "\"Other forms of Employee Share Ownership (ESOW) Plan\": [\"\"],\n",
    "\"Club\": [\"\"],\n",
    "\"Personal membership to gym/ fitness centre/ sports club/ union\": [\"\"],\n",
    "\"Handphone/ Internet reimbursement\": [\"\"],\n",
    "\"Handphone allowance\": [\"\"],\n",
    "\"Cable for TV\": [\"\"],\n",
    "\"Professional bodies\": [\"\"],\n",
    "\"Income tax borne fully or partially by employer\": [\"\"],\n",
    "\"Fixed sum of tax allowance\": [\"\"],\n",
    "\"Subsidies for course fees and training fees for staff development\": [\"\"],\n",
    "\"Scholarship payments\": [\"\"],\n",
    "\"Subsidy of course fees or scholarship given as reward for service rendered\": [\"\"],\n",
    "\"Overseas training\": [\"\"],\n",
    "\"Fixed monthly allowance\": [\"\"],\n",
    "\"Expenses for discharging official duties\": [\"\"],\n",
    "\"Mileage on private cars\": [\"\"],\n",
    "\"Working overtime\": [\"\"],\n",
    "\"Shuttle bus\": [\"\"],\n",
    "\"Taxi trip between home and office\": [\"\"],\n",
    "\"Travel between home and business venue\": [\"\"],\n",
    "\"Trips made by employee between home and external business venues\": [\"\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_for_classification = [\n",
    "\"Tax principle\",\n",
    "\"List of benefits-in-kind granted administrative concession or exempt from income tax\",\n",
    "\"Flexible benefits scheme\",\n",
    "\"Accommodation provided to an employee/director\",\n",
    "\"Furniture and fittings and related benefits\",\n",
    "\"Serviced apartment\",\n",
    "\"Hotel\",\n",
    "\"Housing allowance\",\n",
    "\"Home leave passage\",\n",
    "\"Cash payment in-lieu of home leave passage\",\n",
    "\"Passage provided for business purpose\",\n",
    "\"Passage provided when taking up employment and upon termination\",\n",
    "\"Award for passing exams\",\n",
    "\"Bursary\",\n",
    "\"Innovation/ Improvement\",\n",
    "\"Long service/ retirement\",\n",
    "\"Recognition of good service\",\n",
    "\"Recognition of work performance\",\n",
    "\"Referral\",\n",
    "\"Zero/ low MC\",\n",
    "\"Food, door gifts and lucky draw prizes\",\n",
    "\"Dinner and dance held overseas\",\n",
    "\"Interest benefits arising from interest-free or subsidised interest loan\",\n",
    "\"Interest benefits on loans to company directors\",\n",
    "\"Waiver of principal sum\",\n",
    "\"Car provided\",\n",
    "\"Commercial vehicle provided\",\n",
    "\"Car park charges\",\n",
    "\"ERP charges\",\n",
    "\"Mileage for business usage\",\n",
    "\"Taxes, repairs and maintenance expenses of employee's own vehicle\",\n",
    "\"Subsidy for a child in childcare center\",\n",
    "\"Subsidy for a child in student care\",\n",
    "\"Employer's contributions relating to employment in Singapore\",\n",
    "\"Contributions made from 1 Jan 2004 relating to employment outside Singapore\",\n",
    "\"Contributions relating to director's fees\",\n",
    "\"Festive occasions\",\n",
    "\"Special occasions\",\n",
    "\"Bereavement\",\n",
    "\"Insurance premium\",\n",
    "\"Group medical insurance\",\n",
    "\"Group insurance policy\",\n",
    "\"Travel insurance covering the period of business travel\",\n",
    "\"Work injury compensation\",\n",
    "\"Death gratuities/ Injuries or disability payments/ Workmen compensation\",\n",
    "\"Gratuity for completing number of years of service\",\n",
    "\"Payment to induce a person to join the company\",\n",
    "\"Retrenchment payment to compensate loss of employment\",\n",
    "\"Retirement benefits\",\n",
    "\"Payment made to employee for entering into covenant\",\n",
    "\"Salary in lieu of notice/notice pay\",\n",
    "\"Fixed monthly meal allowance\",\n",
    "\"Working overtime - allowance paid or reimbursement made\",\n",
    "\"Free or subsidised food and drinks\",\n",
    "\"Reimbursement for employees and dependants\",\n",
    "\"Medical benefit based on gender or age\",\n",
    "\"Medical insurance\",\n",
    "\"Transport expenses to see doctor\",\n",
    "\"Absentee payroll under Skills Redevelopment programme\",\n",
    "\"Conditional payments made in advance\",\n",
    "\"Encashment of unutilised leave\",\n",
    "\"Inflation bonus\",\n",
    "\"Laundry allowance\",\n",
    "\"Maternity leave benefit\",\n",
    "\"NSman pay\",\n",
    "\"Skills Development Levy (SDL)\",\n",
    "\"Contributions made by employer to employee's Supplementary Retirement Scheme (SRS) account\",\n",
    "\"Relocation allowance\",\n",
    "\"Contributions made by employer to any pension/provident fund outside Singapore\",\n",
    "\"Employment Assistance Payment (EAP)\",\n",
    "\"Overseas holiday trips\",\n",
    "\"Holiday reimbursement\",\n",
    "\"Overtime allowance\",\n",
    "\"Overtime claims\",\n",
    "\"Per diem allowance\",\n",
    "\"Per diem reimbursement\",\n",
    "\"Combination of allowance and reimbursement\",\n",
    "\"Parking fees at the airport\",\n",
    "\"Travel insurance premium\",\n",
    "\"Travel between home and airport\",\n",
    "\"Payment for warm clothing\",\n",
    "\"Payment for luggage\",\n",
    "\"Facilities owned by employer\",\n",
    "\"Reimbursement for renting chalet\",\n",
    "\"Corporate passes to places of interest\",\n",
    "\"Staff discount offered by employer or its related entities\",\n",
    "\"Staff discount extended to employee's family members, relatives and friends\",\n",
    "\"Employee Share Option (ESOP)\",\n",
    "\"Other forms of Employee Share Ownership (ESOW) Plan\",\n",
    "\"Club\",\n",
    "\"Personal membership to gym/ fitness centre/ sports club/ union\",\n",
    "\"Handphone/ Internet reimbursement\",\n",
    "\"Handphone allowance\",\n",
    "\"Cable for TV\",\n",
    "\"Professional bodies\",\n",
    "\"Income tax borne fully or partially by employer\",\n",
    "\"Fixed sum of tax allowance\",\n",
    "\"Subsidies for course fees and training fees for staff development\",\n",
    "\"Scholarship payments\",\n",
    "\"Subsidy of course fees or scholarship given as reward for service rendered\",\n",
    "\"Overseas training\",\n",
    "\"Fixed monthly allowance\",\n",
    "\"Expenses for discharging official duties\",\n",
    "\"Mileage on private cars\",\n",
    "\"Working overtime\",\n",
    "\"Shuttle bus\",\n",
    "\"Taxi trip between home and office\",\n",
    "\"Travel between home and airport\",\n",
    "\"Travel between home and business venue\",\n",
    "\"Trips made by employee between home and external business venues\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_categories_for_embedding = {\n",
    "'General Information': [\"\"],\n",
    "'Accommodation and Related Benefits': [\"\"],\n",
    "'Air Passage': [\"\"],\n",
    "'Awards': [\"\"],\n",
    "'Benefits relating to Corporate Events': [\"\"],\n",
    "'Benefits relating to Loans': [\"\"],\n",
    "'Car and Car-related Benefits': [\"\"],\n",
    "'Childcare Subsidy': [\"\"],\n",
    "'Central Provident Fund (CPF) Contributions': [\"\"],\n",
    "'Gifts': [\"\"],\n",
    "'Insurance Premium': [\"\"],\n",
    "'Lump Sum Payment': [\"\"],\n",
    "'Meal Payments and Food Provided': [\"\"],\n",
    "'Medical and Dental Care': [\"\"],\n",
    "'Other Payments': [\"\"],\n",
    "'Overseas Holiday Trips': [\"\"],\n",
    "'Overtime Payments': [\"\"],\n",
    "'Per Diem': [\"\"],\n",
    "'Social and Recreational Facilities': [\"\"],\n",
    "'Staff Discount': [\"\"],\n",
    "'Stock Options': [\"\"],\n",
    "'Subscriptions': [\"\"],\n",
    "'Tax Borne by Employer': [\"\"],\n",
    "'Training': [\"\"],\n",
    "'Transport': [\"\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_categories_for_classification = [\n",
    "'General Information',\n",
    "'Accommodation and Related Benefits',\n",
    "'Air Passage',\n",
    "'Awards',\n",
    "'Benefits relating to Corporate Events',\n",
    "'Benefits relating to Loans',\n",
    "'Car and Car-related Benefits',\n",
    "'Childcare Subsidy',\n",
    "'Central Provident Fund (CPF) Contributions',\n",
    "'Gifts',\n",
    "'Insurance Premium',\n",
    "'Lump Sum Payment',\n",
    "'Meal Payments and Food Provided',\n",
    "'Medical and Dental Care',\n",
    "'Other Payments',\n",
    "'Overseas Holiday Trips',\n",
    "'Overtime Payments',\n",
    "'Per Diem',\n",
    "'Social and Recreational Facilities',\n",
    "'Staff Discount',\n",
    "'Stock Options',\n",
    "'Subscriptions',\n",
    "'Tax Borne by Employer',\n",
    "'Training',\n",
    "'Transport'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here for running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run from here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original before made all ' become \" yeaaaa\n",
    "\n",
    "{'Tax principle': [''],\n",
    " 'List of benefits-in-kind granted administrative concession or exempt from income tax': [''],\n",
    " 'Flexible benefits scheme': [''],\n",
    " 'Accommodation provided to an employee/director': [''],\n",
    " 'Furniture and fittings and related benefits': [''],\n",
    " 'Serviced apartment': [''],\n",
    " 'Hotel': [''],\n",
    " 'Housing allowance': [''],\n",
    " 'Home leave passage': [''],\n",
    " 'Cash payment in-lieu of home leave passage': [''],\n",
    " 'Passage provided for business purpose': [''],\n",
    " 'Passage provided when taking up employment and upon termination': [''],\n",
    " 'Award for passing exams': [''],\n",
    " 'Bursary': [''],\n",
    " 'Innovation/ Improvement': [''],\n",
    " 'Long service/ retirement': [''],\n",
    " 'Recognition of good service': [''],\n",
    " 'Recognition of work performance': [''],\n",
    " 'Referral': [''],\n",
    " 'Zero/ low MC': [''],\n",
    " 'Food, door gifts and lucky draw prizes': [''],\n",
    " 'Dinner and dance held overseas': [''],\n",
    " 'Interest benefits arising from interest-free or subsidised interest loan': [''],\n",
    " 'Interest benefits on loans to company directors': [''],\n",
    " 'Waiver of principal sum': [''],\n",
    " 'Car provided': [''],\n",
    " 'Commercial vehicle provided': [''],\n",
    " 'Car park charges': [''],\n",
    " 'ERP charges': [''],\n",
    " 'Mileage for business usage': [''],\n",
    " \"Taxes, repairs and maintenance expenses of employee's own vehicle\": [''],\n",
    " 'Subsidy for a child in childcare center': [''],\n",
    " 'Subsidy for a child in student care': [''],\n",
    " \"Employer's contributions relating to employment in Singapore\": [''],\n",
    " 'Contributions made from 1 Jan 2004 relating to employment outside Singapore': [''],\n",
    " \"Contributions relating to director's fees\": [''],\n",
    " 'Festive occasions': [''],\n",
    " 'Special occasions': [''],\n",
    " 'Bereavement': [''],\n",
    " 'Insurance premium': [''],\n",
    " 'Group medical insurance': [''],\n",
    " 'Group insurance policy': [''],\n",
    " 'Travel insurance covering the period of business travel': [''],\n",
    " 'Work injury compensation': [''],\n",
    " 'Death gratuities/ Injuries or disability payments/ Workmen compensation': [''],\n",
    " 'Gratuity for completing number of years of service': [''],\n",
    " 'Payment to induce a person to join the company': [''],\n",
    " 'Retrenchment payment to compensate loss of employment': [''],\n",
    " 'Retirement benefits': [''],\n",
    " 'Payment made to employee for entering into covenant': [''],\n",
    " 'Salary in lieu of notice/notice pay': [''],\n",
    " 'Fixed monthly meal allowance': [''],\n",
    " 'Working overtime - allowance paid or reimbursement made': [''],\n",
    " 'Free or subsidised food and drinks': [''],\n",
    " 'Reimbursement for employees and dependants': [''],\n",
    " 'Medical benefit based on gender or age': [''],\n",
    " 'Medical insurance': [''],\n",
    " 'Transport expenses to see doctor': [''],\n",
    " 'Absentee payroll under Skills Redevelopment programme': [''],\n",
    " 'Conditional payments made in advance': [''],\n",
    " 'Encashment of unutilised leave': [''],\n",
    " 'Inflation bonus': [''],\n",
    " 'Laundry allowance': [''],\n",
    " 'Maternity leave benefit': [''],\n",
    " 'NSman pay': [''],\n",
    " 'Skills Development Levy (SDL)': [''],\n",
    " \"Contributions made by employer to employee's Supplementary Retirement Scheme (SRS) account\": [''],\n",
    " 'Relocation allowance': [''],\n",
    " 'Contributions made by employer to any pension/provident fund outside Singapore': [''],\n",
    " 'Employment Assistance Payment (EAP)': [''],\n",
    " 'Overseas holiday trips': [''],\n",
    " 'Holiday reimbursement': [''],\n",
    " 'Overtime allowance': [''],\n",
    " 'Overtime claims': [''],\n",
    " 'Per diem allowance': [''],\n",
    " 'Per diem reimbursement': [''],\n",
    " 'Combination of allowance and reimbursement': [''],\n",
    " 'Parking fees at the airport': [''],\n",
    " 'Travel insurance premium': [''],\n",
    " 'Travel between home and airport': [''],\n",
    " 'Payment for warm clothing': [''],\n",
    " 'Payment for luggage': [''],\n",
    " 'Facilities owned by employer': [''],\n",
    " 'Reimbursement for renting chalet': [''],\n",
    " 'Corporate passes to places of interest': [''],\n",
    " 'Staff discount offered by employer or its related entities': [''],\n",
    " \"Staff discount extended to employee's family members, relatives and friends\": [''],\n",
    " 'Employee Share Option (ESOP)': [''],\n",
    " 'Other forms of Employee Share Ownership (ESOW) Plan': [''],\n",
    " 'Club': [''],\n",
    " 'Personal membership to gym/ fitness centre/ sports club/ union': [''],\n",
    " 'Handphone/ Internet reimbursement': [''],\n",
    " 'Handphone allowance': [''],\n",
    " 'Cable for TV': [''],\n",
    " 'Professional bodies': [''],\n",
    " 'Income tax borne fully or partially by employer\\xa0': [''],\n",
    " 'Fixed sum of tax allowance': [''],\n",
    " 'Subsidies for course fees and training fees for staff development': [''],\n",
    " 'Scholarship payments': [''],\n",
    " 'Subsidy of course fees or scholarship given as reward for service rendered': [''],\n",
    " 'Overseas training': [''],\n",
    " 'Fixed monthly allowance': [''],\n",
    " 'Expenses for discharging official duties': [''],\n",
    " 'Mileage on private cars': [''],\n",
    " 'Working overtime': [''],\n",
    " 'Shuttle bus': [''],\n",
    " 'Taxi trip between home and office': [''],\n",
    " 'Travel between home and business venue': [''],\n",
    " 'Trips made by employee between home and external business venues': ['']}\n",
    "\n",
    "\n",
    "['Tax principle',\n",
    " 'List of benefits-in-kind granted administrative concession or exempt from income tax',\n",
    " 'Flexible benefits scheme',\n",
    " 'Accommodation provided to an employee/director',\n",
    " 'Furniture and fittings and related benefits',\n",
    " 'Serviced apartment',\n",
    " 'Hotel',\n",
    " 'Housing allowance',\n",
    " 'Home leave passage',\n",
    " 'Cash payment in-lieu of home leave passage',\n",
    " 'Passage provided for business purpose',\n",
    " 'Passage provided when taking up employment and upon termination',\n",
    " 'Award for passing exams',\n",
    " 'Bursary',\n",
    " 'Innovation/ Improvement',\n",
    " 'Long service/ retirement',\n",
    " 'Recognition of good service',\n",
    " 'Recognition of work performance',\n",
    " 'Referral',\n",
    " 'Zero/ low MC',\n",
    " 'Food, door gifts and lucky draw prizes',\n",
    " 'Dinner and dance held overseas',\n",
    " 'Interest benefits arising from interest-free or subsidised interest loan',\n",
    " 'Interest benefits on loans to company directors',\n",
    " 'Waiver of principal sum',\n",
    " 'Car provided',\n",
    " 'Commercial vehicle provided',\n",
    " 'Car park charges',\n",
    " 'ERP charges',\n",
    " 'Mileage for business usage',\n",
    " \"Taxes, repairs and maintenance expenses of employee's own vehicle\",\n",
    " 'Subsidy for a child in childcare center',\n",
    " 'Subsidy for a child in student care',\n",
    " \"Employer's contributions relating to employment in Singapore\",\n",
    " 'Contributions made from 1 Jan 2004 relating to employment outside Singapore',\n",
    " \"Contributions relating to director's fees\",\n",
    " 'Festive occasions',\n",
    " 'Special occasions',\n",
    " 'Bereavement',\n",
    " 'Insurance premium',\n",
    " 'Group medical insurance',\n",
    " 'Group insurance policy',\n",
    " 'Travel insurance covering the period of business travel',\n",
    " 'Work injury compensation',\n",
    " 'Death gratuities/ Injuries or disability payments/ Workmen compensation',\n",
    " 'Gratuity for completing number of years of service',\n",
    " 'Payment to induce a person to join the company',\n",
    " 'Retrenchment payment to compensate loss of employment',\n",
    " 'Retirement benefits',\n",
    " 'Payment made to employee for entering into covenant',\n",
    " 'Salary in lieu of notice/notice pay',\n",
    " 'Fixed monthly meal allowance',\n",
    " 'Working overtime - allowance paid or reimbursement made',\n",
    " 'Free or subsidised food and drinks',\n",
    " 'Reimbursement for employees and dependants',\n",
    " 'Medical benefit based on gender or age',\n",
    " 'Medical insurance',\n",
    " 'Transport expenses to see doctor',\n",
    " 'Absentee payroll under Skills Redevelopment programme',\n",
    " 'Conditional payments made in advance',\n",
    " 'Encashment of unutilised leave',\n",
    " 'Inflation bonus',\n",
    " 'Laundry allowance',\n",
    " 'Maternity leave benefit',\n",
    " 'NSman pay',\n",
    " 'Skills Development Levy (SDL)',\n",
    " \"Contributions made by employer to employee's Supplementary Retirement Scheme (SRS) account\",\n",
    " 'Relocation allowance',\n",
    " 'Contributions made by employer to any pension/provident fund outside Singapore',\n",
    " 'Employment Assistance Payment (EAP)',\n",
    " 'Overseas holiday trips',\n",
    " 'Holiday reimbursement',\n",
    " 'Overtime allowance',\n",
    " 'Overtime claims',\n",
    " 'Per diem allowance',\n",
    " 'Per diem reimbursement',\n",
    " 'Combination of allowance and reimbursement',\n",
    " 'Parking fees at the airport',\n",
    " 'Travel insurance premium',\n",
    " 'Travel between home and airport',\n",
    " 'Payment for warm clothing',\n",
    " 'Payment for luggage',\n",
    " 'Facilities owned by employer',\n",
    " 'Reimbursement for renting chalet',\n",
    " 'Corporate passes to places of interest',\n",
    " 'Staff discount offered by employer or its related entities',\n",
    " \"Staff discount extended to employee's family members, relatives and friends\",\n",
    " 'Employee Share Option (ESOP)',\n",
    " 'Other forms of Employee Share Ownership (ESOW) Plan',\n",
    " 'Club',\n",
    " 'Personal membership to gym/ fitness centre/ sports club/ union',\n",
    " 'Handphone/ Internet reimbursement',\n",
    " 'Handphone allowance',\n",
    " 'Cable for TV',\n",
    " 'Professional bodies',\n",
    " 'Income tax borne fully or partially by employer\\xa0',\n",
    " 'Fixed sum of tax allowance',\n",
    " 'Subsidies for course fees and training fees for staff development',\n",
    " 'Scholarship payments',\n",
    " 'Subsidy of course fees or scholarship given as reward for service rendered',\n",
    " 'Overseas training',\n",
    " 'Fixed monthly allowance',\n",
    " 'Expenses for discharging official duties',\n",
    " 'Mileage on private cars',\n",
    " 'Working overtime',\n",
    " 'Shuttle bus',\n",
    " 'Taxi trip between home and office',\n",
    " 'Travel between home and airport',\n",
    " 'Travel between home and business venue',\n",
    " 'Trips made by employee between home and external business venues']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sentences\n",
    "\n",
    "sentences = [\"\",\n",
    "             \"\",\n",
    "             \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing In General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Functions Name Obtainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_MiniLM_L6_v2_embedding\n",
      "all_MiniLM_L12_v2_embedding\n",
      "bge_large_v1_5_embedding\n",
      "bart_large_cls_embedding\n",
      "bart_large_mean_embedding\n",
      "g_canine_embedding\n",
      "mxbai_large_v1_embedding\n",
      "all_mpnet_base_v2_embedding\n",
      "paraphrase_MiniLM_L6_v2_embedding\n",
      "paraphrase_multilingual_MiniLM_L12_v2_embedding\n",
      "gte_large_en_v1_5_embedding\n",
      "nomic_embed_text_v1_5_embedding\n",
      "bart_mnli_classifier\n",
      "nli_roberta_base_classifier\n",
      "deberta_v3_base_mnli_fever_anli_classifier\n"
     ]
    }
   ],
   "source": [
    "for i in (embedding_functions_list + classification_functions_list):\n",
    "    print(i.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Splitting Capabilities Amongst Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text = \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_MiniLM_L6_v2_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "\n",
      "all_MiniLM_L12_v2_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "\n",
      "bge_large_v1_5_embedding\n",
      "['Employees receive free flight benefits to', 'return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "\n",
      "bart_large_cls_embedding\n",
      "['Employees receive free flight benefits', 'to return', 'to home country once a year and free flight benefits', 'for business trips']\n",
      "\n",
      "bart_large_mean_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and free flight benefits', 'for business trips']\n",
      "\n",
      "g_canine_embedding\n",
      "['Employees receive free flight benefits to return to home country once a', 'year and free flight benefits for business trips']\n",
      "\n",
      "mxbai_large_v1_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "\n",
      "all_mpnet_base_v2_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and free flight benefits for business trips']\n",
      "\n",
      "paraphrase_MiniLM_L6_v2_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "\n",
      "paraphrase_multilingual_MiniLM_L12_v2_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "\n",
      "gte_large_en_v1_5_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits', 'for business trips']\n",
      "\n",
      "nomic_embed_text_v1_5_embedding\n",
      "['Employees receive free flight benefits', 'to return to home country once a year and', 'free flight benefits for business trips']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for emb_func in embedding_functions_list:\n",
    "    print(emb_func.__name__)\n",
    "    print(semantic_segmentation_function(emb_func, temp_text))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Classification Capabilities Between Multi-Label (True/False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi-Label = True, values will be higher\n",
    "- Multi-Label = False, values will be lower (softmax amongst all thats why, so if want \"compare\", then need compare against not 0.5 or etc, but 1/(no. of categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 0.00909090909090909)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_classification_categories = len(categories_for_classification)\n",
    "avg_prob_of_each_classification_category = 1/no_of_classification_categories\n",
    "no_of_classification_categories, avg_prob_of_each_classification_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 0.04)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_broad_classification_categories = len(broad_categories_for_classification)\n",
    "avg_prob_of_each_broad_classification_category = 1/no_of_broad_classification_categories\n",
    "no_of_broad_classification_categories, avg_prob_of_each_broad_classification_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_classify_top_limit_cleaned(g_canine_embedding, bart_mnli_classifier, categories_for_classification, temp_text, multi_label=True, top_many_cat=5, limit_value=0, extra_clean_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_classify_top_limit_cleaned(g_canine_embedding, bart_mnli_classifier, categories_for_classification, temp_text, multi_label=False, top_many_cat=5, limit_value=0, extra_clean_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_classify_top_limit_cleaned(g_canine_embedding, bart_mnli_classifier, categories_for_classification, temp_text, multi_label=False, top_many_cat=5, limit_value=avg_prob_of_each_classification_category, extra_clean_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_classify_top_limit_cleaned(g_canine_embedding, bart_mnli_classifier, broad_categories_for_classification, temp_text, multi_label=True, top_many_cat=5, limit_value=0, extra_clean_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_classify_top_limit_cleaned(g_canine_embedding, bart_mnli_classifier, broad_categories_for_classification, temp_text, multi_label=True, top_many_cat=-1, limit_value=0, extra_clean_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_classify_top_limit_cleaned(g_canine_embedding, bart_mnli_classifier, broad_categories_for_classification, temp_text, multi_label=False, top_many_cat=5, limit_value=0, extra_clean_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_classify_top_limit_cleaned(g_canine_embedding, bart_mnli_classifier, broad_categories_for_classification, temp_text, multi_label=False, top_many_cat=-1, limit_value=avg_prob_of_each_broad_classification_category, extra_clean_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Similarity Capabilities Amongst Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for emb_func in embedding_functions_list:\n",
    "    print(emb_func.__name__)\n",
    "    split_and_compare_wsub_top_limit_cleaned(g_canine_embedding, emb_func, categories_for_embedding, temp_text, top_many_cat=5, limit_value=-1, extra_clean_output=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "def process_extracted_folder_freetext(extracted_raw_files_folder = \"Extracted Raw Data\"):\n",
    "    form_extracted_data_name = \"form_extracted_data.json\"\n",
    "    \n",
    "    ## not dictionary since no unique key to give/use\n",
    "    free_text_res = {}\n",
    "    \n",
    "    cur_dir = os.path.realpath(\".\")\n",
    "    \n",
    "    ##extracted_raw_files_folder = (\"Extracted \"+ raw_files_folder) ## argument fitted\n",
    "    \n",
    "    extracted_data_folder = os.path.join(cur_dir, extracted_raw_files_folder)\n",
    "    if not os.path.exists(extracted_data_folder):\n",
    "        print(f\"No extract folder to process from!!\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    \n",
    "    companies_folders = next(os.walk((\"./\"+extracted_raw_files_folder)))[1]\n",
    "    #list_of_raw_companies_folders_abs_path = [os.path.join(cur_dir, raw_files_folder, companies_folder) for companies_folder in companies_folders]\n",
    "    for company_folder in companies_folders:\n",
    "        extracted_company_folder_abs_path = os.path.join(cur_dir, extracted_raw_files_folder, company_folder)\n",
    "        extracted_company_data_file_abs_path = os.path.join(extracted_company_folder_abs_path, form_extracted_data_name)\n",
    "        print(extracted_company_data_file_abs_path)\n",
    "        free_text_res[extracted_company_data_file_abs_path] = []\n",
    "        if not os.path.exists(extracted_company_data_file_abs_path):\n",
    "            continue\n",
    "        with open(extracted_company_data_file_abs_path, \"r\") as extracted_company_data_file:\n",
    "            loaded_in_extracted_data = json.load(extracted_company_data_file)\n",
    "        free_text_content_list = loaded_in_extracted_data[\"Free Text\"]\n",
    "        print(free_text_content_list)\n",
    "        \n",
    "        free_text_cats = []\n",
    "        for free_text_content in free_text_content_list:\n",
    "            splitted_text, classify_res = split_and_classify_top_limit_cleaned(all_MiniLM_L6_v2_embedding, bart_mnli_classifier, candidate_possible_labels=categories_for_classification, sentence_text=free_text_content, top_many_cat=5, get_list=True, extra_clean_output=True)\n",
    "            for_output_cat_list = [cat_to_comp_pred_dict[0] for cat_to_comp_pred_dict in classify_res]\n",
    "            for_output_cat_list_trim = for_output_cat_list[:5]\n",
    "            free_text_res[extracted_company_data_file_abs_path].append(for_output_cat_list_trim)\n",
    "            \n",
    "            free_text_cats += for_output_cat_list_trim\n",
    "        \n",
    "        post_process_extracted_company_data_file_abs_path = os.path.join(extracted_company_folder_abs_path, (\"Classified \" + form_extracted_data_name))\n",
    "\n",
    "        with open(post_process_extracted_company_data_file_abs_path, \"w\") as proccesed_extracted_data_file:\n",
    "            loaded_in_extracted_data[\"Classified Free Text Categories\"] = free_text_cats\n",
    "            json.dump(loaded_in_extracted_data, proccesed_extracted_data_file)\n",
    "        \n",
    "        \"\"\"\n",
    "        #print(msg_files_abs_path_list)\n",
    "        count = 0\n",
    "        form_extracted_data_name = \"form_extracted_data.json\"\n",
    "        for msg_file_abs_path in msg_files_abs_path_list:\n",
    "            \n",
    "            count += 1\n",
    "            if count > 1:\n",
    "                form_extracted_data_name = f\"form_extracted_data_{count}.json\"\n",
    "                print(\"Multiple Copies of Msg?!?!\")\n",
    "            extracted_data = extract_data_from_msg_file(msg_file_abs_path)\n",
    "            if extracted_data == None:\n",
    "                continue\n",
    "            processed_extracted_data = process_extracted_data(extracted_data)\n",
    "            #print(processed_extracted_data)\n",
    "            #print()\n",
    "            ## not dictionary since no unique key to give/use\n",
    "            extracted_data_list.append(processed_extracted_data)\n",
    "            output_extracted_file(extracted_raw_company_folder_abs_path, form_extracted_data_name, processed_extracted_data)\n",
    "        \"\"\"\n",
    "    return free_text_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_extracted_folder_freetext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maybe top x(up to, since not every point got value) for EACH POINT, better? not overall?\n",
    "## join together with other one maybe too\n",
    "\n",
    "## but need pick model, else too time consuming unless can do on server?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_extracted_folder_freetext(extracted_raw_files_folder = \"Extracted Companies Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb_func in embedding_functions_list:\n",
    "    print(emb_func.__name__)\n",
    "    print(semantic_segmentation_function(emb_func, 'Long service award. Newborn Award. Wedding award.'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_extracted_folder_freetext() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "def process_extracted_folder_freetext(extracted_raw_files_folder = \"Extracted Raw Data\"):\n",
    "    form_extracted_data_name = \"form_extracted_data.json\"\n",
    "    \n",
    "    ## not dictionary since no unique key to give/use\n",
    "    free_text_res = {}\n",
    "    \n",
    "    cur_dir = os.path.realpath(\".\")\n",
    "    \n",
    "    ##extracted_raw_files_folder = (\"Extracted \"+ raw_files_folder) ## argument fitted\n",
    "    \n",
    "    extracted_data_folder = os.path.join(cur_dir, extracted_raw_files_folder)\n",
    "    if not os.path.exists(extracted_data_folder):\n",
    "        print(f\"No extract folder to process from!!\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    \n",
    "    companies_folders = next(os.walk((\"./\"+extracted_raw_files_folder)))[1]\n",
    "    #list_of_raw_companies_folders_abs_path = [os.path.join(cur_dir, raw_files_folder, companies_folder) for companies_folder in companies_folders]\n",
    "    for company_folder in companies_folders:\n",
    "        extracted_company_folder_abs_path = os.path.join(cur_dir, extracted_raw_files_folder, company_folder)\n",
    "        extracted_company_data_file_abs_path = os.path.join(extracted_company_folder_abs_path, form_extracted_data_name)\n",
    "        print(extracted_company_data_file_abs_path)\n",
    "        free_text_res[extracted_company_data_file_abs_path] = []\n",
    "        if not os.path.exists(extracted_company_data_file_abs_path):\n",
    "            continue\n",
    "        with open(extracted_company_data_file_abs_path, \"r\") as extracted_company_data_file:\n",
    "            loaded_in_extracted_data = json.load(extracted_company_data_file)\n",
    "        free_text_content_list = loaded_in_extracted_data[\"Free Text\"]\n",
    "        print(free_text_content_list)\n",
    "        \n",
    "        free_text_cats = []\n",
    "        for free_text_content in free_text_content_list:\n",
    "            splitted_text, classify_res = split_and_classify_top_limit_cleaned(all_MiniLM_L6_v2_embedding, bart_mnli_classifier, candidate_possible_labels=categories_for_classification, sentence_text=free_text_content, top_many_cat=5, get_list=True, extra_clean_output=True)\n",
    "            for_output_cat_list = [cat_to_comp_pred_dict[0] for cat_to_comp_pred_dict in classify_res]\n",
    "            for_output_cat_list_trim = for_output_cat_list[:5]\n",
    "            free_text_res[extracted_company_data_file_abs_path].append(for_output_cat_list_trim)\n",
    "            \n",
    "            free_text_cats += for_output_cat_list_trim\n",
    "        \n",
    "        post_process_extracted_company_data_file_abs_path = os.path.join(extracted_company_folder_abs_path, (\"Classified \" + form_extracted_data_name))\n",
    "\n",
    "        with open(post_process_extracted_company_data_file_abs_path, \"w\") as proccesed_extracted_data_file:\n",
    "            loaded_in_extracted_data[\"Classified Free Text Categories\"] = free_text_cats\n",
    "            json.dump(loaded_in_extracted_data, proccesed_extracted_data_file)\n",
    "        \n",
    "        \"\"\"\n",
    "        #print(msg_files_abs_path_list)\n",
    "        count = 0\n",
    "        form_extracted_data_name = \"form_extracted_data.json\"\n",
    "        for msg_file_abs_path in msg_files_abs_path_list:\n",
    "            \n",
    "            count += 1\n",
    "            if count > 1:\n",
    "                form_extracted_data_name = f\"form_extracted_data_{count}.json\"\n",
    "                print(\"Multiple Copies of Msg?!?!\")\n",
    "            extracted_data = extract_data_from_msg_file(msg_file_abs_path)\n",
    "            if extracted_data == None:\n",
    "                continue\n",
    "            processed_extracted_data = process_extracted_data(extracted_data)\n",
    "            #print(processed_extracted_data)\n",
    "            #print()\n",
    "            ## not dictionary since no unique key to give/use\n",
    "            extracted_data_list.append(processed_extracted_data)\n",
    "            output_extracted_file(extracted_raw_company_folder_abs_path, form_extracted_data_name, processed_extracted_data)\n",
    "        \"\"\"\n",
    "    return free_text_res\n",
    "\n",
    "#process_extracted_folder_freetext()\n",
    "\n",
    "## maybe top x(up to, since not every point got value) for EACH POINT, better? not overall?\n",
    "## join together with other one maybe too\n",
    "\n",
    "## but need pick model, else too time consuming unless can do on server?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OverallProgram() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getopt\n",
    "def OverallProgram():\n",
    "    extracted_raw_files_folder = \"Extracted Raw Data\"\n",
    "    opts, argss = getopt.getopt(sys.argv[1:], \"e:\") ## split by pair of 2s\n",
    "    for opt, val in opts:\n",
    "        if opt == \"-e\":\n",
    "            extracted_raw_files_folder = val.strip(\".\\\\\").strip('\"')\n",
    "    if not os.path.exists(os.path.join(os.path.realpath(\"./\"), extracted_raw_files_folder)):\n",
    "        print(f\"The path '{os.path.join(os.path.realpath(\"./\"), extracted_raw_files_folder)}' does not exists?!?!\")\n",
    "        print(\"Have a extracted-companies-files-overall-folder named 'Extracted Raw Data'\")\n",
    "        print(\"OR\")\n",
    "        print(\"Usage: \" + sys.argv[0] + \" -e extracted-companies-files-overall-folder\")\n",
    "        sys.exit(1)\n",
    "    return process_extracted_folder_freetext(extracted_raw_files_folder=extracted_raw_files_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OverallProgram()\n",
    "#process_extracted_folder_freetext(extracted_raw_files_folder = \"Extracted (Raw/Companies) Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking and Ensuring the import modules can be imported and install if not installed yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "used_modules = [\"torch\", \"math\", \"numpy\", \"itertools\", \"matplotlib\", \"scipy\", \"functools\", \"transformers\", \"sentence_transformers\", \"json\"]\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "def setup_modules(used_modules):\n",
    "    missing_modules = []\n",
    "\n",
    "    for mod in used_modules:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except ModuleNotFoundError:\n",
    "            missing_modules.append(mod)\n",
    "    for mod in missing_modules:\n",
    "        if mod == \"win32com.client\":\n",
    "            install(\"pywin32\")\n",
    "        elif mod == \"torch\":\n",
    "            package_list = [\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"]\n",
    "            subprocess.check_call(([sys.executable, \"-m\", \"pip3\", \"install\"] + package_list))\n",
    "        else:\n",
    "            install(mod)\n",
    "    #print(f\"Please re-run the program, some packages were installed\")\n",
    "    #sys.exit(1)\n",
    "    if len(missing_modules) != 0:\n",
    "        os.execv(sys.executable, ['python'] + sys.argv)\n",
    "setup_modules(used_modules)\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import numpy\n",
    "import itertools\n",
    "import matplotlib\n",
    "import scipy\n",
    "import functools\n",
    "import transformers\n",
    "import sentence_transformers\n",
    "import json\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-r', '.\\\\aa b c\\\\', '-e', 'vv bb\\\\']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "temp_regex_test = r'-r .\\aa b c\" -e vv bb\"'\n",
    "new_temp_regex_test, sub_count = re.subn(r'\\\"', r\"\\\\\", temp_regex_test)\n",
    "\n",
    "#re.sub(r\"( ?)(-[re])( ?)\", r\"*\\2*\", temp_regex_test), re.sub(r'\\\"', r\"\\\\\", temp_regex_test)\n",
    "re.sub(r\"( ?)(-[re])( ?)\", r\"*\\2*\", re.sub(r'\\\"', r\"\\\\\", temp_regex_test))\n",
    "#re.split(r\" ?(-[re]) ?\", temp_regex_test),[string_content for string_content in re.split(r\" ?(-[re]) ?\", temp_regex_test) if string_content] , re.sub(r'\\\"', r\"\\\\\", temp_regex_test)\n",
    "[string_c for string_c in re.split(r\" ?(-[re]) ?\", re.sub(r'\\\"', r\"\\\\\", temp_regex_test)) if string_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859f391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9af3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d31626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c8745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e447d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6b931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab997cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc01c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General random pre testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted texts: ['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "Category: Combination of allowance and reimbursement\n",
      "Employees receive free flight benefits                           : 0.98542\n",
      "\n",
      "Category: Staff discount offered by employer or its related entities\n",
      "Employees receive free flight benefits                           : 0.97565\n",
      "\n",
      "Category: Flexible benefits scheme\n",
      "free flight benefits for business trips                          : 0.97337\n",
      "\n",
      "Category: Passage provided for business purpose\n",
      "Employees receive free flight benefits                           : 0.96443\n",
      "\n",
      "Category: Travel between home and business venue\n",
      "Employees receive free flight benefits                           : 0.9213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_and_classify_top_limit_cleaned(all_MiniLM_L12_v2_embedding, bart_mnli_classifier, categories_for_classification, \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\", top_many_cat=5, limit_value=0.6, extra_clean_output=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted texts: ['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "Category: Combination of allowance and reimbursement\n",
      "Employees receive free flight benefits                           : 0.98542\n",
      "\n",
      "Category: Staff discount offered by employer or its related entities\n",
      "Employees receive free flight benefits                           : 0.97565\n",
      "\n",
      "Category: Flexible benefits scheme\n",
      "free flight benefits for business trips                          : 0.97337\n",
      "\n",
      "Category: Passage provided for business purpose\n",
      "Employees receive free flight benefits                           : 0.96443\n",
      "\n",
      "Category: Travel between home and business venue\n",
      "Employees receive free flight benefits                           : 0.9213\n",
      "\n",
      "Category: Home leave passage\n",
      "to return to home country once                                   : 0.91937\n",
      "\n",
      "Category: List of benefits-in-kind granted administrative concession or exempt from income tax\n",
      "Employees receive free flight benefits                           : 0.90943\n",
      "\n",
      "Category: Travel between home and airport\n",
      "Employees receive free flight benefits                           : 0.89749\n",
      "\n",
      "Category: Trips made by employee between home and external business venues\n",
      "Employees receive free flight benefits                           : 0.89267\n",
      "\n",
      "Category: Reimbursement for employees and dependants\n",
      "Employees receive free flight benefits                           : 0.83841\n",
      "\n",
      "Category: Referral\n",
      "to return to home country once                                   : 0.81322\n",
      "\n",
      "Category: Professional bodies\n",
      "free flight benefits for business trips                          : 0.76407\n",
      "\n",
      "Category: Employer's contributions relating to employment in Singapore\n",
      "free flight benefits for business trips                          : 0.75369\n",
      "\n",
      "Category: Conditional payments made in advance\n",
      "Employees receive free flight benefits                           : 0.74301\n",
      "\n",
      "Category: Mileage for business usage\n",
      "free flight benefits for business trips                          : 0.73609\n",
      "\n",
      "Category: Recognition of good service\n",
      "Employees receive free flight benefits                           : 0.72794\n",
      "\n",
      "Category: Special occasions\n",
      "to return to home country once                                   : 0.71959\n",
      "\n",
      "Category: Facilities owned by employer\n",
      "free flight benefits for business trips                          : 0.62278\n",
      "\n",
      "Category: Overseas training\n",
      "Employees receive free flight benefits                           : 0.62234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_and_classify_top_limit_cleaned(all_MiniLM_L12_v2_embedding, bart_mnli_classifier, categories_for_classification, \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\", top_many_cat=-1, limit_value=0.6, extra_clean_output=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted texts: ['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "Category: Combination of allowance and reimbursement\n",
      "Employees receive free flight benefits                           : 0.98542\n",
      "\n",
      "Category: Staff discount offered by employer or its related entities\n",
      "Employees receive free flight benefits                           : 0.97565\n",
      "\n",
      "Category: Flexible benefits scheme\n",
      "free flight benefits for business trips                          : 0.97337\n",
      "\n",
      "Category: Passage provided for business purpose\n",
      "Employees receive free flight benefits                           : 0.96443\n",
      "\n",
      "Category: Travel between home and business venue\n",
      "Employees receive free flight benefits                           : 0.9213\n",
      "\n",
      "Category: Home leave passage\n",
      "to return to home country once                                   : 0.91937\n",
      "\n",
      "Category: List of benefits-in-kind granted administrative concession or exempt from income tax\n",
      "Employees receive free flight benefits                           : 0.90943\n",
      "\n",
      "Category: Travel between home and airport\n",
      "Employees receive free flight benefits                           : 0.89749\n",
      "\n",
      "Category: Trips made by employee between home and external business venues\n",
      "Employees receive free flight benefits                           : 0.89267\n",
      "\n",
      "Category: Reimbursement for employees and dependants\n",
      "Employees receive free flight benefits                           : 0.83841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_and_classify_top_limit_cleaned(all_MiniLM_L12_v2_embedding, bart_mnli_classifier, categories_for_classification, \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\", top_many_cat=-1, limit_value=0.8, extra_clean_output=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted texts: ['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "Category: Combination of allowance and reimbursement\n",
      "Employees receive free flight benefits                           : 0.98542\n",
      "\n",
      "Category: Staff discount offered by employer or its related entities\n",
      "Employees receive free flight benefits                           : 0.97565\n",
      "\n",
      "Category: Flexible benefits scheme\n",
      "free flight benefits for business trips                          : 0.97337\n",
      "\n",
      "Category: Passage provided for business purpose\n",
      "Employees receive free flight benefits                           : 0.96443\n",
      "\n",
      "Category: Travel between home and business venue\n",
      "Employees receive free flight benefits                           : 0.9213\n",
      "\n",
      "Category: Home leave passage\n",
      "to return to home country once                                   : 0.91937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_and_classify_top_limit_cleaned(all_MiniLM_L12_v2_embedding, bart_mnli_classifier, categories_for_classification, \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\", top_many_cat=-1, limit_value=0.9, extra_clean_output=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted texts: ['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n"
     ]
    }
   ],
   "source": [
    "split_and_classify_top_limit_cleaned(all_MiniLM_L12_v2_embedding, bart_mnli_classifier, categories_for_classification, \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\", top_many_cat=-1, limit_value=0.9, extra_clean_output=True, multi_label=False)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted texts: ['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n"
     ]
    }
   ],
   "source": [
    "split_and_classify_top_limit_cleaned(all_MiniLM_L12_v2_embedding, bart_mnli_classifier, categories_for_classification, \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\", top_many_cat=-1, limit_value=0.5, extra_clean_output=True, multi_label=False)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted texts: ['Employees receive free flight benefits', 'to return to home country once', 'a year and', 'free flight benefits for business trips']\n",
      "Category: Home leave passage\n",
      "to return to home country once                                   : 0.17272\n",
      "\n",
      "Category: Combination of allowance and reimbursement\n",
      "free flight benefits for business trips                          : 0.15723\n",
      "\n",
      "Category: Passage provided for business purpose\n",
      "free flight benefits for business trips                          : 0.11797\n",
      "\n",
      "Category: Staff discount offered by employer or its related entities\n",
      "Employees receive free flight benefits                           : 0.10499\n",
      "\n",
      "Category: Referral\n",
      "to return to home country once                                   : 0.096944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_and_classify_top_limit_cleaned(all_MiniLM_L12_v2_embedding, bart_mnli_classifier, categories_for_classification, \"Employees receive free flight benefits to return to home country once a year and free flight benefits for business trips\", top_many_cat=5, limit_value=0, extra_clean_output=True, multi_label=False)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'free flight benefits for business trips',\n",
       " 'labels': ['Combination of allowance and reimbursement',\n",
       "  'Passage provided for business purpose',\n",
       "  'Flexible benefits scheme',\n",
       "  'Mileage for business usage',\n",
       "  'List of benefits-in-kind granted administrative concession or exempt from income tax',\n",
       "  'Trips made by employee between home and external business venues',\n",
       "  'Travel between home and business venue',\n",
       "  'Recognition of good service',\n",
       "  'Staff discount offered by employer or its related entities',\n",
       "  'Conditional payments made in advance',\n",
       "  'Professional bodies',\n",
       "  'Recognition of work performance',\n",
       "  'Reimbursement for employees and dependants',\n",
       "  'Travel between home and airport',\n",
       "  'Travel between home and airport',\n",
       "  'Zero/ low MC',\n",
       "  'Encashment of unutilised leave',\n",
       "  \"Employer's contributions relating to employment in Singapore\",\n",
       "  'Facilities owned by employer',\n",
       "  'Working overtime - allowance paid or reimbursement made',\n",
       "  'Club',\n",
       "  'Waiver of principal sum',\n",
       "  'Corporate passes to places of interest',\n",
       "  'Travel insurance covering the period of business travel',\n",
       "  'Referral',\n",
       "  'Special occasions',\n",
       "  'Innovation/ Improvement',\n",
       "  'Car provided',\n",
       "  'Overtime claims',\n",
       "  'Per diem reimbursement',\n",
       "  'Commercial vehicle provided',\n",
       "  'Bursary',\n",
       "  'Group insurance policy',\n",
       "  'NSman pay',\n",
       "  'Home leave passage',\n",
       "  'Employment Assistance Payment (EAP)',\n",
       "  'Per diem allowance',\n",
       "  'Contributions made by employer to any pension/provident fund outside Singapore',\n",
       "  'Accommodation provided to an employee/director',\n",
       "  'Cash payment in-lieu of home leave passage',\n",
       "  'Overseas training',\n",
       "  'Taxi trip between home and office',\n",
       "  'Overtime allowance',\n",
       "  'Fixed sum of tax allowance',\n",
       "  'Payment made to employee for entering into covenant',\n",
       "  'Income tax borne fully or partially by employer',\n",
       "  'Other forms of Employee Share Ownership (ESOW) Plan',\n",
       "  'Employee Share Option (ESOP)',\n",
       "  'Salary in lieu of notice/notice pay',\n",
       "  'Serviced apartment',\n",
       "  'Relocation allowance',\n",
       "  'Holiday reimbursement',\n",
       "  'Absentee payroll under Skills Redevelopment programme',\n",
       "  \"Contributions relating to director's fees\",\n",
       "  'Dinner and dance held overseas',\n",
       "  'Hotel',\n",
       "  'Expenses for discharging official duties',\n",
       "  'Work injury compensation',\n",
       "  \"Contributions made by employer to employee's Supplementary Retirement Scheme (SRS) account\",\n",
       "  'Skills Development Levy (SDL)',\n",
       "  'Fixed monthly allowance',\n",
       "  'Inflation bonus',\n",
       "  'Passage provided when taking up employment and upon termination',\n",
       "  'Payment for luggage',\n",
       "  \"Taxes, repairs and maintenance expenses of employee's own vehicle\",\n",
       "  'Shuttle bus',\n",
       "  'Contributions made from 1 Jan 2004 relating to employment outside Singapore',\n",
       "  'Travel insurance premium',\n",
       "  'Interest benefits arising from interest-free or subsidised interest loan',\n",
       "  'ERP charges',\n",
       "  'Handphone allowance',\n",
       "  'Group medical insurance',\n",
       "  'Retrenchment payment to compensate loss of employment',\n",
       "  'Furniture and fittings and related benefits',\n",
       "  'Transport expenses to see doctor',\n",
       "  'Reimbursement for renting chalet',\n",
       "  \"Staff discount extended to employee's family members, relatives and friends\",\n",
       "  'Bereavement',\n",
       "  'Fixed monthly meal allowance',\n",
       "  'Tax principle',\n",
       "  'Payment to induce a person to join the company',\n",
       "  'Death gratuities/ Injuries or disability payments/ Workmen compensation',\n",
       "  'Scholarship payments',\n",
       "  'Handphone/ Internet reimbursement',\n",
       "  'Mileage on private cars',\n",
       "  'Gratuity for completing number of years of service',\n",
       "  'Insurance premium',\n",
       "  'Free or subsidised food and drinks',\n",
       "  'Subsidy for a child in student care',\n",
       "  'Festive occasions',\n",
       "  'Long service/ retirement',\n",
       "  'Subsidy of course fees or scholarship given as reward for service rendered',\n",
       "  'Interest benefits on loans to company directors',\n",
       "  'Medical benefit based on gender or age',\n",
       "  'Medical insurance',\n",
       "  'Subsidy for a child in childcare center',\n",
       "  'Working overtime',\n",
       "  'Subsidies for course fees and training fees for staff development',\n",
       "  'Housing allowance',\n",
       "  'Payment for warm clothing',\n",
       "  'Car park charges',\n",
       "  'Award for passing exams',\n",
       "  'Cable for TV',\n",
       "  'Personal membership to gym/ fitness centre/ sports club/ union',\n",
       "  'Parking fees at the airport',\n",
       "  'Retirement benefits',\n",
       "  'Laundry allowance',\n",
       "  'Food, door gifts and lucky draw prizes',\n",
       "  'Maternity leave benefit',\n",
       "  'Overseas holiday trips'],\n",
       " 'scores': [0.15722674131393433,\n",
       "  0.11796680837869644,\n",
       "  0.08571615815162659,\n",
       "  0.05669112503528595,\n",
       "  0.03373070806264877,\n",
       "  0.030603602528572083,\n",
       "  0.025500038638710976,\n",
       "  0.020055623725056648,\n",
       "  0.020046204328536987,\n",
       "  0.019550221040844917,\n",
       "  0.01933140866458416,\n",
       "  0.018151823431253433,\n",
       "  0.01810515858232975,\n",
       "  0.015520470216870308,\n",
       "  0.015520470216870308,\n",
       "  0.015122109092772007,\n",
       "  0.015007898211479187,\n",
       "  0.01418815366923809,\n",
       "  0.013680270873010159,\n",
       "  0.011500720866024494,\n",
       "  0.011381966061890125,\n",
       "  0.011242248117923737,\n",
       "  0.010793402791023254,\n",
       "  0.01020459458231926,\n",
       "  0.008823706768453121,\n",
       "  0.008800550363957882,\n",
       "  0.008425424806773663,\n",
       "  0.0077512445859611034,\n",
       "  0.007580357138067484,\n",
       "  0.007190776988863945,\n",
       "  0.00689684646204114,\n",
       "  0.006676461081951857,\n",
       "  0.0065805334597826,\n",
       "  0.006577573250979185,\n",
       "  0.006145077757537365,\n",
       "  0.005840812344104052,\n",
       "  0.0058051664382219315,\n",
       "  0.005332787521183491,\n",
       "  0.005171673372387886,\n",
       "  0.00472803832963109,\n",
       "  0.0046270499005913734,\n",
       "  0.00446129497140646,\n",
       "  0.004205635748803616,\n",
       "  0.004105761181563139,\n",
       "  0.004055703990161419,\n",
       "  0.0035190933849662542,\n",
       "  0.003475748933851719,\n",
       "  0.0032017529010772705,\n",
       "  0.003150314325466752,\n",
       "  0.0031498197931796312,\n",
       "  0.0031372858211398125,\n",
       "  0.0030522814486175776,\n",
       "  0.002780453534796834,\n",
       "  0.0026322307530790567,\n",
       "  0.0026210269425064325,\n",
       "  0.00259539601393044,\n",
       "  0.0025253265630453825,\n",
       "  0.002488435013219714,\n",
       "  0.0022762715816497803,\n",
       "  0.0022261859849095345,\n",
       "  0.0022112291771918535,\n",
       "  0.0021721376106142998,\n",
       "  0.002065727487206459,\n",
       "  0.001889510778710246,\n",
       "  0.0018798252567648888,\n",
       "  0.0018350619357079268,\n",
       "  0.001825488405302167,\n",
       "  0.0017840953078120947,\n",
       "  0.0017714303685352206,\n",
       "  0.0017644878244027495,\n",
       "  0.0016970561118796468,\n",
       "  0.0015749395824968815,\n",
       "  0.00151515391189605,\n",
       "  0.0014968272298574448,\n",
       "  0.001440546941012144,\n",
       "  0.0014154602540656924,\n",
       "  0.0013710629427805543,\n",
       "  0.0013697434915229678,\n",
       "  0.001342914765700698,\n",
       "  0.0013373087858781219,\n",
       "  0.001331336796283722,\n",
       "  0.0012748421868309379,\n",
       "  0.0011756852036342025,\n",
       "  0.0011695751454681158,\n",
       "  0.001071083010174334,\n",
       "  0.0010630941251292825,\n",
       "  0.0010583491530269384,\n",
       "  0.0010194649221375585,\n",
       "  0.0010187879670411348,\n",
       "  0.0010134916519746184,\n",
       "  0.0009870262583717704,\n",
       "  0.0009703885298222303,\n",
       "  0.0009416350512765348,\n",
       "  0.0009044295293278992,\n",
       "  0.0008383513777516782,\n",
       "  0.0008257707813754678,\n",
       "  0.0007981074741110206,\n",
       "  0.0007834119605831802,\n",
       "  0.0007754953112453222,\n",
       "  0.0007693992229178548,\n",
       "  0.0007609587628394365,\n",
       "  0.0007581089739687741,\n",
       "  0.0007432554266415536,\n",
       "  0.0007343962788581848,\n",
       "  0.0007206022855825722,\n",
       "  0.0007015661685727537,\n",
       "  0.0006841815775260329,\n",
       "  0.0006384827429428697,\n",
       "  0.0006317474762909114,\n",
       "  0.0006249211728572845]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_mnli_classifier(\"free flight benefits for business trips\", categories_for_classification, multi_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
